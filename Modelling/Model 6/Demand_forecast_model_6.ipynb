{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryyutku/DSGP/blob/anuk/Modelling/Model%206/Demand_forecast_model_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVZorFFc5JYb"
      },
      "source": [
        "# **Training model withoutdifferencing or any other augmentation and transformations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RArxWlkIzQxN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-bFQph84Lmx"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('CIEC.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpv5b8Sy4PWI"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26aduZCw4VqU"
      },
      "outputs": [],
      "source": [
        "df['date'] = pd.to_datetime(df['date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dje-tQJD4lJI"
      },
      "outputs": [],
      "source": [
        "features = ['date','fuel_consumption', 'petroleum_imports_crudeOil',\n",
        "       'Taxes_on_Customs_and_Other_Import Duties',\n",
        "       'Foreign Direct Investments', 'GDP Goods and Services',\n",
        "       'GDP: Gross National Income', 'Government Debt',\n",
        "       'New Vehicle Registrations', 'Vehicle Sales', 'Port Stay Duration',\n",
        "       'Vehicle Sales Asia', 'No.of Vessels Colombo',\n",
        "       'Imports of Refined Products', 'Colombo port calls',\n",
        "       'Tax income profits_gains', 'Tax on Export', 'Tax Goods & Services',\n",
        "       'Tax Road Transport', 'GDP FCE Households', 'Diesel User Price',\n",
        "       'Petrol User Price', 'Consumption_Oil', 'Sales 90 Octane',\n",
        "       'Sales 95 Octane', 'Sales Auto Diesel', 'Household_income',\n",
        "       'Fuel_other_manufacture']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ur9qshjBynz"
      },
      "outputs": [],
      "source": [
        "df_original = df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMacLCXn66AP"
      },
      "source": [
        "## **Checking the time frame with the most columns available**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e5H-XQw45Bd"
      },
      "outputs": [],
      "source": [
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-WGHDmD5FAd"
      },
      "outputs": [],
      "source": [
        "len(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwZQsfhwfbkk"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H82YSENmcD3m"
      },
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP6ZF9tgddnl"
      },
      "source": [
        "## **Outlier detection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrydtKhDdgTe"
      },
      "outputs": [],
      "source": [
        "#Detecting outliers using Zscore\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import zscore\n",
        "\n",
        "def detect_outliers_zscore(df, feature, threshold=2, time_column='date'):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Skip non-numeric columns\n",
        "    if not np.issubdtype(df[feature].dtype, np.number):\n",
        "        print(f\"Skipping non-numeric column: {feature}\")\n",
        "        return None, df  # Skip non-numeric columns\n",
        "\n",
        "    # Drop rows with missing values in the feature column\n",
        "    df = df.dropna(subset=[feature])\n",
        "\n",
        "    # Calculate Z-scores for the feature\n",
        "    df['zscore'] = zscore(df[feature])\n",
        "\n",
        "    # Identify anomalies based on the Z-score threshold\n",
        "    df[feature + '_isanomaly'] = (df['zscore'].abs() > threshold)\n",
        "\n",
        "    # Filter anomaly data\n",
        "    anomalies = df[df[feature + '_isanomaly']]\n",
        "\n",
        "    # Select numerical output\n",
        "    anomalies = anomalies[[time_column, feature, 'zscore', feature + '_isanomaly']]\n",
        "\n",
        "    return anomalies, df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2HBS3hbdiBn"
      },
      "outputs": [],
      "source": [
        "processed_df = df.copy()\n",
        "\n",
        "for feature in df.columns:\n",
        "    print(\"----\", feature, \"----\")\n",
        "    anomalies, processed_df = detect_outliers_zscore(processed_df, feature=feature, threshold=2)\n",
        "\n",
        "    if anomalies is not None:\n",
        "        print(f\"Feature: {feature}\")\n",
        "        print(anomalies.head())\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cS8Bls0XtWD7"
      },
      "outputs": [],
      "source": [
        "processed_df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DFYyyJzrosa"
      },
      "source": [
        "### **Removing the outliers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDT5vBU2rtxA"
      },
      "outputs": [],
      "source": [
        "processed_df = processed_df[processed_df['New Vehicle Registrations_isanomaly'] == False]\n",
        "processed_df = processed_df[processed_df['No.of Vessels Colombo_isanomaly'] == False]\n",
        "processed_df = processed_df[processed_df['Imports of Refined Products_isanomaly'] == False]\n",
        "processed_df = processed_df[processed_df['Tax Road Transport_isanomaly'] == False]\n",
        "processed_df = processed_df[processed_df['Petrol User Price_isanomaly'] == False]\n",
        "processed_df = processed_df[processed_df['Sales 90 Octane_isanomaly'] == False]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KBep7fruQSQ"
      },
      "source": [
        "### **Test running a model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYrIXw0nvW8I"
      },
      "outputs": [],
      "source": [
        "# dropping cols\n",
        "processed_df = processed_df.drop(columns=[col for col in processed_df.columns if '_isanomaly' in col])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rARWx8S3u6nK"
      },
      "outputs": [],
      "source": [
        "# Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Define features to scale (exclude 'date' and 'fuel_consumption' if it's the target)\n",
        "features_to_scale = processed_df.drop(columns=['date', 'fuel_consumption']).columns\n",
        "\n",
        "scaler = StandardScaler()\n",
        "processed_df[features_to_scale] = scaler.fit_transform(processed_df[features_to_scale])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ya1PdJePvpI6"
      },
      "outputs": [],
      "source": [
        "pip install pycaret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kzpF20RGvaPG",
        "outputId": "caf4ea72-c9ee-43d9-cbde-ed1db13e6b75"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "You must pass a freq argument as current index has none.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-5ee2eb62697f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Initialize PyCaret for time series\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m ts_exp = setup(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprocessed_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fuel_consumption'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pycaret/time_series/forecasting/functional.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(data, data_func, target, index, ignore_features, numeric_imputation_target, numeric_imputation_exogenous, transform_target, transform_exogenous, fe_target_rr, fe_exogenous, scale_target, scale_exogenous, fold_strategy, fold, fh, hyperparameter_split, seasonal_period, ignore_seasonality_test, sp_detection, max_sp_to_consider, remove_harmonics, harmonic_order_method, num_sps_to_use, seasonality_type, point_alpha, coverage, enforce_exogenous, n_jobs, use_gpu, custom_pipeline, html, session_id, system_log, log_experiment, experiment_name, log_plots, log_profile, log_data, verbose, profile, profile_kwargs, fig_kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_EXPERIMENT_CLASS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0mset_current_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m     return exp.setup(\n\u001b[0m\u001b[1;32m    591\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mdata_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pycaret/time_series/forecasting/oop.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, data, data_func, target, index, ignore_features, numeric_imputation_target, numeric_imputation_exogenous, transform_target, transform_exogenous, scale_target, scale_exogenous, fe_target_rr, fe_exogenous, fold_strategy, fold, fh, hyperparameter_split, seasonal_period, ignore_seasonality_test, sp_detection, max_sp_to_consider, remove_harmonics, harmonic_order_method, num_sps_to_use, seasonality_type, point_alpha, coverage, enforce_exogenous, n_jobs, use_gpu, custom_pipeline, html, session_id, system_log, log_experiment, experiment_name, experiment_custom_tags, log_plots, log_profile, log_data, engine, verbose, profile, profile_kwargs, fig_kwargs)\u001b[0m\n\u001b[1;32m   2105\u001b[0m             )\n\u001b[1;32m   2106\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0m_check_clean_and_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2107\u001b[0;31m             \u001b[0;34m.\u001b[0m\u001b[0m_check_and_clean_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseasonal_period\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseasonal_period\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2108\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0m_check_and_set_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2109\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0m_set_exogenous_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pycaret/time_series/forecasting/oop.py\u001b[0m in \u001b[0;36m_check_and_clean_index\u001b[0;34m(self, index, seasonal_period)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# Ref: https://github.com/sktime/sktime/blob/v0.10.0/sktime/forecasting/base/_fh.py#L524\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatetimeIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_period\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;31m# Data must not have missing indices ----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/extension.py\u001b[0m in \u001b[0;36mmethod\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"inplace\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"cannot use inplace with {type(self).__name__}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/datetimes.py\u001b[0m in \u001b[0;36mto_period\u001b[0;34m(self, freq)\u001b[0m\n\u001b[1;32m   1210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1212\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   1213\u001b[0m                     \u001b[0;34m\"You must pass a freq argument as current index has none.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 )\n",
            "\u001b[0;31mValueError\u001b[0m: You must pass a freq argument as current index has none."
          ]
        }
      ],
      "source": [
        "from pycaret.time_series import *\n",
        "\n",
        "# Initialize PyCaret for time series\n",
        "ts_exp = setup(\n",
        "    processed_df,\n",
        "    target='fuel_consumption',\n",
        "    index='date',\n",
        "    session_id=123\n",
        ")\n",
        "\n",
        "# Compare models\n",
        "best_model = compare_models()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZB4zbF2-cLKy"
      },
      "source": [
        "## **ADF Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spbZ4vnpcKTh"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "def stationary_test(df, feature):\n",
        "    df = df.copy()\n",
        "    result = adfuller(feature.dropna())  # ADF test result\n",
        "    adf_results = {\n",
        "        'ADF Statistic': result[0],\n",
        "        'p-value': result[1],\n",
        "        'Critical Values': result[4]\n",
        "    }\n",
        "\n",
        "    # Determine if series is stationary based on the ADF test\n",
        "    if result[1] < 0.05:  # If p-value < 0.05, we reject the null hypothesis (non-stationary)\n",
        "        is_stationary = True\n",
        "    else:\n",
        "        is_stationary = False\n",
        "\n",
        "    # Checking if ADF statistic is less than critical values at 5% level\n",
        "    if result[0] < adf_results['Critical Values']['5%']:\n",
        "        stationary_check = True  # Rejects the null hypothesis\n",
        "    else:\n",
        "        stationary_check = False  # Fails to reject the null hypothesis\n",
        "\n",
        "    # Output results with the determination of stationarity\n",
        "    print(f\"ADF Statistic: {result[0]}\")\n",
        "    print(f\"p-value: {result[1]}\")\n",
        "    print(f\"Critical Values: {result[4]}\")\n",
        "    print(f\"Is Stationary Based on p-value: {is_stationary}\")\n",
        "    print(f\"Is Stationary Based on ADF Statistic vs Critical Value: {stationary_check}\")\n",
        "    print()\n",
        "\n",
        "    return adf_results, is_stationary, stationary_check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3e0Bun5ggSyb"
      },
      "outputs": [],
      "source": [
        "for feature in df:\n",
        "  print(f\"Performing stationary test for {feature}\")\n",
        "  adf_results = stationary_test(df,df[feature])\n",
        "  print(adf_results)\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppKoSDWygafr"
      },
      "source": [
        "Need to perform differences on the data as it is a non-stationary dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnlqsXELgeQj"
      },
      "source": [
        "## **Rolling statistics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AzTQNPrFgj5g"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def rolling_stats(df, feature, window=4):\n",
        "    df = df.copy()  # Ensure df is properly copied\n",
        "    if df[feature].dtype not in ['int64', 'float64']:  # Skip non-numeric columns\n",
        "        print(f\"Skipping {feature}: Not a numeric column\")\n",
        "        return\n",
        "\n",
        "    df[feature + \"_rmean\"] = df[feature].rolling(window=window).mean()\n",
        "\n",
        "    # Plot original feature and rolling mean\n",
        "    df[[feature, feature + \"_rmean\"]].plot(figsize=(6, 3), title=\"Rolling Mean of \" + feature)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IYu4jQMqgqpv"
      },
      "outputs": [],
      "source": [
        "# Iterate only over numeric columns\n",
        "for feature in df.select_dtypes(include=['number']).columns:\n",
        "    rolling_stats(df, feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAuMEdT5g3rp"
      },
      "source": [
        "## **ACF/PACF Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1CoKI5lGhDCN"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.stattools import acf,pacf\n",
        "def acf_pacf(df,feature):\n",
        "  acf_values = acf(df[feature].dropna(), nlags=10)\n",
        "  pacf_values = pacf(df[feature].dropna(), nlags=10)\n",
        "\n",
        "  acf_df = pd.DataFrame({'Lag':range(11), 'ACF':acf_values})\n",
        "  pacf_df = pd.DataFrame({'Lag':range(11), 'PACF':pacf_values})\n",
        "  print(\"ACF and PACF for \",feature)\n",
        "  print(\"Autocorrelation Values:\")\n",
        "  print(acf_df)\n",
        "  print(\"\\nPartial Autocorrelation Values:\")\n",
        "  print(pacf_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3WdwVD3EhEAL"
      },
      "outputs": [],
      "source": [
        "for feature in df.columns:\n",
        "  acf_pacf(df,feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyvf08Syhau4"
      },
      "source": [
        "## **ACF/PACF Strength test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7Uxo-HSFheIW"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.stattools import acf, pacf\n",
        "import numpy as np\n",
        "\n",
        "def acf_pacf_strength(df, feature, nlags=10):\n",
        "    acf_values = acf(df[feature].dropna(), nlags=nlags)\n",
        "    pacf_values = pacf(df[feature].dropna(), nlags=nlags)\n",
        "\n",
        "    # Compute how quickly ACF declines (higher means strong trend)\n",
        "    acf_strength = np.sum(np.abs(acf_values[1:])) / nlags  # Ignore lag 0 (always 1)\n",
        "\n",
        "    # Check if PACF drops off after the first lag (higher means trend)\n",
        "    pacf_strength = abs(pacf_values[1])  # PACF at lag 1 shows direct correlation\n",
        "\n",
        "    print(f\"{feature}: ACF Strength = {acf_strength:.4f}, PACF Strength = {pacf_strength:.4f}\")\n",
        "\n",
        "    return acf_strength, pacf_strength\n",
        "\n",
        "# Run for all columns\n",
        "for feature in df.columns:\n",
        "    acf_pacf_strength(df, feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rxwki4QMhnxx"
      },
      "source": [
        "## **Lag Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a8IhDOQ7hnZ0"
      },
      "outputs": [],
      "source": [
        "def lag_analysis(df,feature):\n",
        "  df[feature+'lag1'] = df[feature].shift(1)\n",
        "\n",
        "  # Show numerical correlation\n",
        "  lag_corr = df[[feature,feature+'lag1']].corr().iloc[0,1]\n",
        "  print(\"Correlation between\",feature,\"and its 1-day lag:\",lag_corr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vvzRxVzZhszq"
      },
      "outputs": [],
      "source": [
        "for feature in df.columns:\n",
        "  lag_analysis(df,feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PjwuJx06rDF7"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ4IHz1hh20Y"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hY7vaRbSh48K"
      },
      "outputs": [],
      "source": [
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CXN2uUCRiEhc"
      },
      "outputs": [],
      "source": [
        "# Extracting date feature\n",
        "df['year'] = df['date'].dt.year\n",
        "df['month'] = df['date'].dt.month\n",
        "df['day'] = df['date'].dt.day\n",
        "df['weekday'] = df['date'].dt.weekday\n",
        "df['quarter'] = df['date'].dt.quarter\n",
        "\n",
        "df.drop('date',axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGfswsjK_hPE"
      },
      "source": [
        "## **Scaling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kb2KuQEEihsX"
      },
      "outputs": [],
      "source": [
        "# Feature scaling\n",
        "numerical_cols = df.select_dtypes(include=['float64','int64']).columns\n",
        "numerical_cols = numerical_cols.drop('fuel_consumption')\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
        "\n",
        "fuel_scaler = StandardScaler()\n",
        "df['fuel_consumption'] = fuel_scaler.fit_transform(df[['fuel_consumption']])\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CiXFFxiZq1Oa"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"data.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lN2TV9zXq8ED"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tdJZe1qOi3yj"
      },
      "outputs": [],
      "source": [
        "# splitting data into feature and target variables\n",
        "X = df.drop('fuel_consumption',axis=1)\n",
        "y = df['fuel_consumption']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Yc5T8WrsjASF"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state= 42)\n",
        "print(\"Training shape\",X_train.shape)\n",
        "print(\"Test df shape\",X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TVEEW0EJjT4g"
      },
      "outputs": [],
      "source": [
        "# visualize correlation matrix\n",
        "corr = df.corr()\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm',fmt='.2f')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZX7aa0QSjqND"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0mdROIUpmEK1"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_rf = rf_model.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6lUlsQASnIrO"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "# Using Random Forest (replace with y_pred_lr for Linear Regression)\n",
        "predictions = y_pred_rf  # Replace with y_pred_lr if you're using Linear Regression\n",
        "\n",
        "# Create traces for actual and predicted values\n",
        "trace_actual = go.Scatter(\n",
        "    x=y_test.index, y=y_test, mode='lines+markers', name='Actual Fuel Demand', line=dict(color='blue')\n",
        ")\n",
        "\n",
        "trace_predicted = go.Scatter(\n",
        "    x=y_test.index, y=predictions, mode='lines+markers', name='Predicted Fuel Demand', line=dict(color='red', dash='dash')\n",
        ")\n",
        "\n",
        "# Create the layout for the plot\n",
        "layout = go.Layout(\n",
        "    title='Actual vs Predicted Fuel Demand (Test Set)',\n",
        "    xaxis=dict(title='Index (Test Set)'),\n",
        "    yaxis=dict(title='Fuel Demand'),\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "# Plot the figure\n",
        "fig = go.Figure(data=[trace_actual, trace_predicted], layout=layout)\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_rIu07YWnyuL"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Calculate the evaluation metrics\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "# Print out all the metrics\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
        "print(f'R-squared (R²): {r2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PUD2PkSdqU5E"
      },
      "outputs": [],
      "source": [
        "# Inverse transform on predicted and actual values\n",
        "predicted_values = fuel_scaler.inverse_transform(predictions.reshape(-1, 1))\n",
        "actual_values = fuel_scaler.inverse_transform(y_test.values.reshape(-1, 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "km11JEZwAXEz"
      },
      "outputs": [],
      "source": [
        "print(y_test.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "P3P6dD55A-VL"
      },
      "outputs": [],
      "source": [
        "# Retrieve corresponding dates from the original dataset\n",
        "y_test_dates = df_original.loc[y_test.index, 'date']  # Replace 'df_original' with the original dataframe before dropping 'date'\n",
        "\n",
        "# Convert predictions and actual values back to original scale\n",
        "predictions = predicted_values.flatten()  # Use inverse-transformed predictions\n",
        "actuals = actual_values.flatten()  # Use inverse-transformed actual values\n",
        "\n",
        "# Create traces for actual and predicted values\n",
        "trace_actual = go.Scatter(\n",
        "    x=y_test_dates, y=actuals, mode='lines+markers', name='Actual Fuel Demand', line=dict(color='blue')\n",
        ")\n",
        "\n",
        "trace_predicted = go.Scatter(\n",
        "    x=y_test_dates, y=predictions, mode='lines+markers', name='Predicted Fuel Demand', line=dict(color='red', dash='dash')\n",
        ")\n",
        "\n",
        "# Create the layout for the plot\n",
        "layout = go.Layout(\n",
        "    title='Actual vs Predicted Fuel Demand (Test Set)',\n",
        "    xaxis=dict(title='Date', tickformat=\"%Y-%m-%d\"),  # Format dates properly\n",
        "    yaxis=dict(title='Fuel Demand (Original Scale)'),\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "# Plot the figure\n",
        "fig = go.Figure(data=[trace_actual, trace_predicted], layout=layout)\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8ofk2A4IDfTz"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "# Create figure\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add actual values as scatter plot (points only)\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=y_test_dates, y=actuals, mode='markers',\n",
        "    name='Actual Fuel Demand', marker=dict(color='blue', size=8, symbol='circle')\n",
        "))\n",
        "\n",
        "# Add predicted values as scatter plot (points only)\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=y_test_dates, y=predictions, mode='markers',\n",
        "    name='Predicted Fuel Demand', marker=dict(color='red', size=8, symbol='x')\n",
        "))\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(title=\"Actual vs Predicted Fuel Demand (Test Set)\",\n",
        "                  xaxis_title=\"Date\", yaxis_title=\"Fuel Demand\",\n",
        "                  xaxis=dict(showgrid=False),\n",
        "                  yaxis=dict(showgrid=True, zeroline=False),\n",
        "                  showlegend=True)\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knUfJR9KEVMg"
      },
      "source": [
        "## **Predicting future demand**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "izXHpQpmSCyi"
      },
      "outputs": [],
      "source": [
        "# Save the trained model as fuel_demand.pkl\n",
        "with open(\"fuel_demand.pkl\", \"wb\") as f:\n",
        "    pickle.dump(rf_model, f)\n",
        "\n",
        "print(\"Model saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "k-olUdMlIfiB"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "c7UeVsUik1EJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNB2y8hJnSKTeHN3TX1D9Pr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}