{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "import optuna\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, Baseline\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from pytorch_forecasting.data.encoders import NaNLabelEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/yehana2002/Projects/DSGP/datasets/processed/final_merged_dataset_ready.csv\")\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df = df.sort_values(\"Date\").reset_index(drop=True)\n",
    "df[\"series_id\"] = \"sri_lanka\"\n",
    "df.rename(columns={\"Petrol_Price\": \"petrol_price\"}, inplace=True)\n",
    "df[\"time_idx\"] = np.arange(len(df))\n",
    "\n",
    "df.fillna(method=\"ffill\", inplace=True)\n",
    "df.fillna(method=\"bfill\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_length = 36\n",
    "max_prediction_length = 12\n",
    "training_cutoff = df[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "time_varying_known_reals = [\n",
    "    \"time_idx\", \"Date\"\n",
    "] + [col for col in df.columns if col not in [\"petrol_price\", \"diesel_price\", \"series_id\", \"time_idx\"] and df[col].dtype in [np.float64, np.int64]]\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    df[df.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"petrol_price\",\n",
    "    group_ids=[\"series_id\"],\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"series_id\"],\n",
    "    time_varying_known_reals=time_varying_known_reals,\n",
    "    time_varying_unknown_reals=[\"petrol_price\"],\n",
    "    target_normalizer=GroupNormalizer(groups=[\"series_id\"]),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(training, df, predict=True, stop_randomization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = TimeSeriesDataSet.from_dataset(training, df, predict=True, stop_randomization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optuna testing for Petrol model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 8, 64)\n",
    "    attention_head_size = trial.suggest_int(\"attention_head_size\", 1, 4)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.3)\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)\n",
    "\n",
    "    train_loader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "    val_loader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    model = TemporalFusionTransformer.from_dataset(\n",
    "        training,\n",
    "        hidden_size=hidden_size,\n",
    "        attention_head_size=attention_head_size,\n",
    "        dropout=dropout,\n",
    "        learning_rate=learning_rate,\n",
    "        loss=SMAPE(),\n",
    "        log_interval=0,\n",
    "        reduce_on_plateau_patience=4,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        max_epochs=20,\n",
    "        accelerator=\"cpu\",\n",
    "        logger=False,\n",
    "        enable_checkpointing=False,\n",
    "        callbacks=[EarlyStopping(monitor=\"val_loss\", patience=5)],\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "    val_actuals = torch.cat([y[0] for x, y in iter(val_loader)])\n",
    "    val_predictions = model.predict(val_loader)\n",
    "\n",
    "    smape_score = SMAPE()(val_predictions, val_actuals).item()\n",
    "    return smape_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:30:49,447]\u001b[0m A new study created in memory with name: TFT_Petrol_Tuning\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 5.0 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 164 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 161 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 14.8 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 14.8 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 14.8 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 14.8 K\n",
      "11 | lstm_encoder                       | LSTM                            | 29.3 K\n",
      "12 | lstm_decoder                       | LSTM                            | 29.3 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 7.3 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 120   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 18.4 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 9.7 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 7.4 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 14.8 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 7.4 K \n",
      "20 | output_layer                       | Linear                          | 61    \n",
      "----------------------------------------------------------------------------------------\n",
      "512 K     Trainable params\n",
      "0         Non-trainable params\n",
      "512 K     Total params\n",
      "2.051     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2/2 [00:00<00:00,  5.78it/s, loss=0.54, train_loss_step=0.540, val_loss=0.993, train_loss_epoch=0.540] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:30:53,153]\u001b[0m Trial 0 finished with value: 0.992513120174408 and parameters: {'batch_size': 32, 'hidden_size': 60, 'attention_head_size': 3, 'dropout': 0.06831979376511693, 'learning_rate': 0.0003010774807632081}. Best is trial 0 with value: 0.992513120174408.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 5.1 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 166 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 164 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 15.2 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 15.2 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 15.2 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 15.2 K\n",
      "11 | lstm_encoder                       | LSTM                            | 30.3 K\n",
      "12 | lstm_decoder                       | LSTM                            | 30.3 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 7.6 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 122   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 19.0 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 9.9 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 7.7 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 15.2 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 7.7 K \n",
      "20 | output_layer                       | Linear                          | 62    \n",
      "----------------------------------------------------------------------------------------\n",
      "523 K     Trainable params\n",
      "0         Non-trainable params\n",
      "523 K     Total params\n",
      "2.096     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00,  6.05it/s, loss=0.39, train_loss_step=0.338, val_loss=0.197, train_loss_epoch=0.338] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00,  6.03it/s, loss=0.39, train_loss_step=0.338, val_loss=0.197, train_loss_epoch=0.338]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:31:00,066]\u001b[0m Trial 1 finished with value: 0.19725435972213745 and parameters: {'batch_size': 32, 'hidden_size': 61, 'attention_head_size': 3, 'dropout': 0.15767956900492577, 'learning_rate': 0.001522494827357329}. Best is trial 1 with value: 0.19725435972213745.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 5.0 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 164 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 161 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 14.8 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 14.8 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 14.8 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 14.8 K\n",
      "11 | lstm_encoder                       | LSTM                            | 29.3 K\n",
      "12 | lstm_decoder                       | LSTM                            | 29.3 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 7.3 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 120   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 18.4 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 11.0 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 7.4 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 14.8 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 7.4 K \n",
      "20 | output_layer                       | Linear                          | 61    \n",
      "----------------------------------------------------------------------------------------\n",
      "513 K     Trainable params\n",
      "0         Non-trainable params\n",
      "513 K     Total params\n",
      "2.055     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 7/7 [00:00<00:00,  7.11it/s, loss=0.0874, train_loss_step=0.0872, val_loss=0.101, train_loss_epoch=0.083]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:31:15,544]\u001b[0m Trial 2 finished with value: 0.10094732791185379 and parameters: {'batch_size': 8, 'hidden_size': 60, 'attention_head_size': 2, 'dropout': 0.08543085990762918, 'learning_rate': 0.001743171631944831}. Best is trial 2 with value: 0.10094732791185379.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.3 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 101 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 100 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 5.4 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 5.4 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 5.4 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 5.4 K \n",
      "11 | lstm_encoder                       | LSTM                            | 10.7 K\n",
      "12 | lstm_decoder                       | LSTM                            | 10.7 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.7 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 72    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 6.7 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.3 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.7 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 5.4 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.7 K \n",
      "20 | output_layer                       | Linear                          | 37    \n",
      "----------------------------------------------------------------------------------------\n",
      "270 K     Trainable params\n",
      "0         Non-trainable params\n",
      "270 K     Total params\n",
      "1.082     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 2/2 [00:00<00:00,  7.29it/s, loss=0.318, train_loss_step=0.311, val_loss=0.663, train_loss_epoch=0.311]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:31:17,347]\u001b[0m Trial 3 finished with value: 0.6634525656700134 and parameters: {'batch_size': 32, 'hidden_size': 36, 'attention_head_size': 4, 'dropout': 0.06923545567392277, 'learning_rate': 0.00045432455087031435}. Best is trial 2 with value: 0.10094732791185379.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 2.7 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 81.3 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 80.2 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 3.3 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 3.3 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 3.3 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 3.3 K \n",
      "11 | lstm_encoder                       | LSTM                            | 6.5 K \n",
      "12 | lstm_decoder                       | LSTM                            | 6.5 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 1.6 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 56    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 4.1 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 2.0 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 1.7 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 3.3 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 1.7 K \n",
      "20 | output_layer                       | Linear                          | 29    \n",
      "----------------------------------------------------------------------------------------\n",
      "203 K     Trainable params\n",
      "0         Non-trainable params\n",
      "203 K     Total params\n",
      "0.815     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 4/4 [00:00<00:00,  7.92it/s, loss=0.457, train_loss_step=0.576, val_loss=1.140, train_loss_epoch=0.464]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 4/4 [00:00<00:00,  7.91it/s, loss=0.457, train_loss_step=0.576, val_loss=1.140, train_loss_epoch=0.464]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:31:27,685]\u001b[0m Trial 4 finished with value: 1.1403995752334595 and parameters: {'batch_size': 16, 'hidden_size': 28, 'attention_head_size': 4, 'dropout': 0.16866939562964037, 'learning_rate': 0.0003537470110461809}. Best is trial 2 with value: 0.10094732791185379.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.8 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 122 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 120 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 8.0 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 8.0 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 8.0 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 8.0 K \n",
      "11 | lstm_encoder                       | LSTM                            | 15.8 K\n",
      "12 | lstm_decoder                       | LSTM                            | 15.8 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 4.0 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 88    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 9.9 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 5.0 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 4.0 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 8.0 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 4.0 K \n",
      "20 | output_layer                       | Linear                          | 45    \n",
      "----------------------------------------------------------------------------------------\n",
      "344 K     Trainable params\n",
      "0         Non-trainable params\n",
      "344 K     Total params\n",
      "1.377     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00,  7.95it/s, loss=0.198, train_loss_step=0.185, val_loss=0.307, train_loss_epoch=0.194]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00,  7.94it/s, loss=0.198, train_loss_step=0.185, val_loss=0.307, train_loss_epoch=0.194]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:31:45,581]\u001b[0m Trial 5 finished with value: 0.30723097920417786 and parameters: {'batch_size': 8, 'hidden_size': 44, 'attention_head_size': 3, 'dropout': 0.08238156756428876, 'learning_rate': 0.0001667021204175059}. Best is trial 2 with value: 0.10094732791185379.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.0 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 93.9 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 92.7 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 4.6 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.6 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.6 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 4.6 K \n",
      "11 | lstm_encoder                       | LSTM                            | 9.0 K \n",
      "12 | lstm_decoder                       | LSTM                            | 9.0 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.2 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 66    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 5.6 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 2.7 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.3 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 4.6 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.3 K \n",
      "20 | output_layer                       | Linear                          | 34    \n",
      "----------------------------------------------------------------------------------------\n",
      "244 K     Trainable params\n",
      "0         Non-trainable params\n",
      "244 K     Total params\n",
      "0.978     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00,  7.84it/s, loss=0.432, train_loss_step=0.449, val_loss=0.095, train_loss_epoch=0.396] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00,  7.83it/s, loss=0.432, train_loss_step=0.449, val_loss=0.095, train_loss_epoch=0.396]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:32:03,112]\u001b[0m Trial 6 finished with value: 0.09503764659166336 and parameters: {'batch_size': 8, 'hidden_size': 33, 'attention_head_size': 4, 'dropout': 0.28069702592619544, 'learning_rate': 0.0004770915875383292}. Best is trial 6 with value: 0.09503764659166336.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.3 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 101 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 100 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 5.4 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 5.4 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 5.4 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 5.4 K \n",
      "11 | lstm_encoder                       | LSTM                            | 10.7 K\n",
      "12 | lstm_decoder                       | LSTM                            | 10.7 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.7 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 72    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 6.7 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.5 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.7 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 5.4 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.7 K \n",
      "20 | output_layer                       | Linear                          | 37    \n",
      "----------------------------------------------------------------------------------------\n",
      "270 K     Trainable params\n",
      "0         Non-trainable params\n",
      "270 K     Total params\n",
      "1.083     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00,  7.41it/s, loss=0.438, train_loss_step=0.311, val_loss=0.765, train_loss_epoch=0.311]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00,  7.38it/s, loss=0.438, train_loss_step=0.311, val_loss=0.765, train_loss_epoch=0.311]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:32:08,750]\u001b[0m Trial 7 finished with value: 0.7651171088218689 and parameters: {'batch_size': 32, 'hidden_size': 36, 'attention_head_size': 3, 'dropout': 0.06671419905573453, 'learning_rate': 0.0035985108957185393}. Best is trial 6 with value: 0.09503764659166336.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.4 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 142 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 141 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 11.1 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 11.1 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 11.1 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 11.1 K\n",
      "11 | lstm_encoder                       | LSTM                            | 22.0 K\n",
      "12 | lstm_decoder                       | LSTM                            | 22.0 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 5.5 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 104   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 13.8 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 11.0 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 5.6 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 11.1 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 5.6 K \n",
      "20 | output_layer                       | Linear                          | 53    \n",
      "----------------------------------------------------------------------------------------\n",
      "428 K     Trainable params\n",
      "0         Non-trainable params\n",
      "428 K     Total params\n",
      "1.715     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00,  6.64it/s, loss=0.682, train_loss_step=0.515, val_loss=0.754, train_loss_epoch=0.515]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00,  6.61it/s, loss=0.682, train_loss_step=0.515, val_loss=0.754, train_loss_epoch=0.515]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:32:14,942]\u001b[0m Trial 8 finished with value: 0.7540280222892761 and parameters: {'batch_size': 32, 'hidden_size': 52, 'attention_head_size': 1, 'dropout': 0.2850560250427807, 'learning_rate': 0.0025041356329041555}. Best is trial 6 with value: 0.09503764659166336.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.0 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 127 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 125 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 8.7 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 8.7 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 8.7 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 8.7 K \n",
      "11 | lstm_encoder                       | LSTM                            | 17.3 K\n",
      "12 | lstm_decoder                       | LSTM                            | 17.3 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 4.3 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 92    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 10.9 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 8.6 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 4.4 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 8.7 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 4.4 K \n",
      "20 | output_layer                       | Linear                          | 47    \n",
      "----------------------------------------------------------------------------------------\n",
      "366 K     Trainable params\n",
      "0         Non-trainable params\n",
      "366 K     Total params\n",
      "1.467     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00,  6.28it/s, loss=0.289, train_loss_step=0.189, val_loss=0.074, train_loss_epoch=0.189] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00,  6.26it/s, loss=0.289, train_loss_step=0.189, val_loss=0.074, train_loss_epoch=0.189]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:32:21,128]\u001b[0m Trial 9 finished with value: 0.07404226064682007 and parameters: {'batch_size': 32, 'hidden_size': 46, 'attention_head_size': 1, 'dropout': 0.016984507858361652, 'learning_rate': 0.0051786282162447584}. Best is trial 9 with value: 0.07404226064682007.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 1.5 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 41.4 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 40.8 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 648   \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 648   \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 648   \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 648   \n",
      "11 | lstm_encoder                       | LSTM                            | 1.2 K \n",
      "12 | lstm_decoder                       | LSTM                            | 1.2 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 312   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 24    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 792   \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 612   \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 336   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 648   \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 336   \n",
      "20 | output_layer                       | Linear                          | 13    \n",
      "----------------------------------------------------------------------------------------\n",
      "90.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "90.8 K    Total params\n",
      "0.363     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 4/4 [00:00<00:00,  8.75it/s, loss=0.127, train_loss_step=0.104, val_loss=0.143, train_loss_epoch=0.106] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 4/4 [00:00<00:00,  8.73it/s, loss=0.127, train_loss_step=0.104, val_loss=0.143, train_loss_epoch=0.106]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:32:30,586]\u001b[0m Trial 10 finished with value: 0.1430806964635849 and parameters: {'batch_size': 16, 'hidden_size': 12, 'attention_head_size': 1, 'dropout': 0.01414472549602333, 'learning_rate': 0.008814242273767326}. Best is trial 9 with value: 0.07404226064682007.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 2.4 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 71.2 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 70.2 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 2.4 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 2.4 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 2.4 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 2.4 K \n",
      "11 | lstm_encoder                       | LSTM                            | 4.8 K \n",
      "12 | lstm_decoder                       | LSTM                            | 4.8 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 1.2 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 48    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 3.0 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.8 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 1.2 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 2.4 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 1.2 K \n",
      "20 | output_layer                       | Linear                          | 25    \n",
      "----------------------------------------------------------------------------------------\n",
      "173 K     Trainable params\n",
      "0         Non-trainable params\n",
      "173 K     Total params\n",
      "0.692     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00,  8.60it/s, loss=0.308, train_loss_step=0.281, val_loss=0.507, train_loss_epoch=0.297]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00,  8.59it/s, loss=0.308, train_loss_step=0.281, val_loss=0.507, train_loss_epoch=0.297]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:32:47,165]\u001b[0m Trial 11 finished with value: 0.507379949092865 and parameters: {'batch_size': 8, 'hidden_size': 24, 'attention_head_size': 2, 'dropout': 0.29072251124779946, 'learning_rate': 0.0006623188960949435}. Best is trial 9 with value: 0.07404226064682007.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.0 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 127 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 125 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 8.7 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 8.7 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 8.7 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 8.7 K \n",
      "11 | lstm_encoder                       | LSTM                            | 17.3 K\n",
      "12 | lstm_decoder                       | LSTM                            | 17.3 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 4.3 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 92    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 10.9 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 8.6 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 4.4 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 8.7 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 4.4 K \n",
      "20 | output_layer                       | Linear                          | 47    \n",
      "----------------------------------------------------------------------------------------\n",
      "366 K     Trainable params\n",
      "0         Non-trainable params\n",
      "366 K     Total params\n",
      "1.467     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 7/7 [00:00<00:00,  7.62it/s, loss=0.119, train_loss_step=0.163, val_loss=0.0737, train_loss_epoch=0.124] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:33:02,221]\u001b[0m Trial 12 finished with value: 0.0737147256731987 and parameters: {'batch_size': 8, 'hidden_size': 46, 'attention_head_size': 1, 'dropout': 0.2198350495765554, 'learning_rate': 0.009752857730131799}. Best is trial 12 with value: 0.0737147256731987.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.0 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 127 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 125 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 8.7 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 8.7 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 8.7 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 8.7 K \n",
      "11 | lstm_encoder                       | LSTM                            | 17.3 K\n",
      "12 | lstm_decoder                       | LSTM                            | 17.3 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 4.3 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 92    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 10.9 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 8.6 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 4.4 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 8.7 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 4.4 K \n",
      "20 | output_layer                       | Linear                          | 47    \n",
      "----------------------------------------------------------------------------------------\n",
      "366 K     Trainable params\n",
      "0         Non-trainable params\n",
      "366 K     Total params\n",
      "1.467     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 7/7 [00:00<00:00,  7.64it/s, loss=0.137, train_loss_step=0.166, val_loss=0.234, train_loss_epoch=0.131] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:33:10,713]\u001b[0m Trial 13 finished with value: 0.23411524295806885 and parameters: {'batch_size': 8, 'hidden_size': 46, 'attention_head_size': 1, 'dropout': 0.21813604241417742, 'learning_rate': 0.009797561824920968}. Best is trial 12 with value: 0.0737147256731987.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.0 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 129 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 128 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 9.1 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 9.1 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 9.1 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 9.1 K \n",
      "11 | lstm_encoder                       | LSTM                            | 18.0 K\n",
      "12 | lstm_decoder                       | LSTM                            | 18.0 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 4.5 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 94    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 11.3 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 6.6 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 4.6 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 9.1 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 4.6 K \n",
      "20 | output_layer                       | Linear                          | 48    \n",
      "----------------------------------------------------------------------------------------\n",
      "374 K     Trainable params\n",
      "0         Non-trainable params\n",
      "374 K     Total params\n",
      "1.498     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 4/4 [00:00<00:00,  6.45it/s, loss=0.203, train_loss_step=0.165, val_loss=0.230, train_loss_epoch=0.177] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:33:20,188]\u001b[0m Trial 14 finished with value: 0.22979922592639923 and parameters: {'batch_size': 16, 'hidden_size': 47, 'attention_head_size': 2, 'dropout': 0.21923949949961447, 'learning_rate': 0.0048909210952282865}. Best is trial 12 with value: 0.0737147256731987.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.4 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 142 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 141 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 11.1 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 11.1 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 11.1 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 11.1 K\n",
      "11 | lstm_encoder                       | LSTM                            | 22.0 K\n",
      "12 | lstm_decoder                       | LSTM                            | 22.0 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 5.5 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 104   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 13.8 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 11.0 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 5.6 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 11.1 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 5.6 K \n",
      "20 | output_layer                       | Linear                          | 53    \n",
      "----------------------------------------------------------------------------------------\n",
      "428 K     Trainable params\n",
      "0         Non-trainable params\n",
      "428 K     Total params\n",
      "1.715     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00,  6.35it/s, loss=0.29, train_loss_step=0.210, val_loss=0.216, train_loss_epoch=0.210] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00,  6.33it/s, loss=0.29, train_loss_step=0.210, val_loss=0.216, train_loss_epoch=0.210]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:33:26,884]\u001b[0m Trial 15 finished with value: 0.21592526137828827 and parameters: {'batch_size': 32, 'hidden_size': 52, 'attention_head_size': 1, 'dropout': 0.2135503191539309, 'learning_rate': 0.0055509521041481585}. Best is trial 12 with value: 0.0737147256731987.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.8 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 119 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 117 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 7.7 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 7.7 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 7.7 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 7.7 K \n",
      "11 | lstm_encoder                       | LSTM                            | 15.1 K\n",
      "12 | lstm_decoder                       | LSTM                            | 15.1 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 3.8 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 86    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 9.5 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 5.5 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 3.9 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 7.7 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 3.9 K \n",
      "20 | output_layer                       | Linear                          | 44    \n",
      "----------------------------------------------------------------------------------------\n",
      "335 K     Trainable params\n",
      "0         Non-trainable params\n",
      "335 K     Total params\n",
      "1.341     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 7/7 [00:00<00:00,  7.56it/s, loss=0.138, train_loss_step=0.0847, val_loss=0.0714, train_loss_epoch=0.124]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:33:37,248]\u001b[0m Trial 16 finished with value: 0.07138101011514664 and parameters: {'batch_size': 8, 'hidden_size': 43, 'attention_head_size': 2, 'dropout': 0.018800840854248935, 'learning_rate': 0.006352980994277746}. Best is trial 16 with value: 0.07138101011514664.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 1.9 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 53.8 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 53.0 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.3 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.3 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.3 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.3 K \n",
      "11 | lstm_encoder                       | LSTM                            | 2.4 K \n",
      "12 | lstm_decoder                       | LSTM                            | 2.4 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 612   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 34    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.5 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 856   \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 646   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.3 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 646   \n",
      "20 | output_layer                       | Linear                          | 18    \n",
      "----------------------------------------------------------------------------------------\n",
      "123 K     Trainable params\n",
      "0         Non-trainable params\n",
      "123 K     Total params\n",
      "0.492     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00,  8.90it/s, loss=0.233, train_loss_step=0.281, val_loss=0.0735, train_loss_epoch=0.223]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00,  8.89it/s, loss=0.233, train_loss_step=0.281, val_loss=0.0735, train_loss_epoch=0.223]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:33:53,313]\u001b[0m Trial 17 finished with value: 0.0734911635518074 and parameters: {'batch_size': 8, 'hidden_size': 17, 'attention_head_size': 2, 'dropout': 0.1190569807410693, 'learning_rate': 0.0010546027260870802}. Best is trial 16 with value: 0.07138101011514664.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 1.4 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 36.5 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 36.0 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 460   \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 460   \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 460   \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 460   \n",
      "11 | lstm_encoder                       | LSTM                            | 880   \n",
      "12 | lstm_decoder                       | LSTM                            | 880   \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 220   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 20    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 560   \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 325   \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 240   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 460   \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 240   \n",
      "20 | output_layer                       | Linear                          | 11    \n",
      "----------------------------------------------------------------------------------------\n",
      "78.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "78.4 K    Total params\n",
      "0.314     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00,  9.50it/s, loss=0.321, train_loss_step=0.228, val_loss=0.464, train_loss_epoch=0.308]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00,  9.49it/s, loss=0.321, train_loss_step=0.228, val_loss=0.464, train_loss_epoch=0.308]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:34:08,520]\u001b[0m Trial 18 finished with value: 0.46408772468566895 and parameters: {'batch_size': 8, 'hidden_size': 10, 'attention_head_size': 2, 'dropout': 0.12751255154083868, 'learning_rate': 0.0007924172710805882}. Best is trial 16 with value: 0.07138101011514664.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 2.2 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 63.7 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 62.8 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.9 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.9 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.9 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.9 K \n",
      "11 | lstm_encoder                       | LSTM                            | 3.7 K \n",
      "12 | lstm_decoder                       | LSTM                            | 3.7 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 924   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 42    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 2.3 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.3 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 966   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.9 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 966   \n",
      "20 | output_layer                       | Linear                          | 22    \n",
      "----------------------------------------------------------------------------------------\n",
      "150 K     Trainable params\n",
      "0         Non-trainable params\n",
      "150 K     Total params\n",
      "0.604     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00,  8.58it/s, loss=0.145, train_loss_step=0.109, val_loss=0.0767, train_loss_epoch=0.130] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00,  8.57it/s, loss=0.145, train_loss_step=0.109, val_loss=0.0767, train_loss_epoch=0.130]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:34:25,055]\u001b[0m Trial 19 finished with value: 0.07668229192495346 and parameters: {'batch_size': 8, 'hidden_size': 21, 'attention_head_size': 2, 'dropout': 0.0022176991611039254, 'learning_rate': 0.0011999503901538677}. Best is trial 16 with value: 0.07138101011514664.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 1.9 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 53.8 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 53.0 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.3 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.3 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.3 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.3 K \n",
      "11 | lstm_encoder                       | LSTM                            | 2.4 K \n",
      "12 | lstm_decoder                       | LSTM                            | 2.4 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 612   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 34    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.5 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 856   \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 646   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.3 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 646   \n",
      "20 | output_layer                       | Linear                          | 18    \n",
      "----------------------------------------------------------------------------------------\n",
      "123 K     Trainable params\n",
      "0         Non-trainable params\n",
      "123 K     Total params\n",
      "0.492     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00,  8.88it/s, loss=0.4, train_loss_step=0.362, val_loss=0.718, train_loss_epoch=0.429]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00,  8.87it/s, loss=0.4, train_loss_step=0.362, val_loss=0.718, train_loss_epoch=0.429]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:34:41,082]\u001b[0m Trial 20 finished with value: 0.7175341248512268 and parameters: {'batch_size': 8, 'hidden_size': 17, 'attention_head_size': 2, 'dropout': 0.11715270283636857, 'learning_rate': 0.00012328514372646942}. Best is trial 16 with value: 0.07138101011514664.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.5 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 109 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 107 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 6.3 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 6.3 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 6.3 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 6.3 K \n",
      "11 | lstm_encoder                       | LSTM                            | 12.5 K\n",
      "12 | lstm_decoder                       | LSTM                            | 12.5 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 3.1 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 78    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 7.8 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 6.2 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 3.2 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 6.3 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 3.2 K \n",
      "20 | output_layer                       | Linear                          | 40    \n",
      "----------------------------------------------------------------------------------------\n",
      "299 K     Trainable params\n",
      "0         Non-trainable params\n",
      "299 K     Total params\n",
      "1.198     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 7/7 [00:00<00:00,  7.75it/s, loss=0.126, train_loss_step=0.117, val_loss=0.131, train_loss_epoch=0.116] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:34:54,163]\u001b[0m Trial 21 finished with value: 0.13126392662525177 and parameters: {'batch_size': 8, 'hidden_size': 39, 'attention_head_size': 1, 'dropout': 0.18938002691986822, 'learning_rate': 0.0027592345545139002}. Best is trial 16 with value: 0.07138101011514664.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 2.8 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 86.3 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 85.2 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 3.8 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 3.8 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 3.8 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 3.8 K \n",
      "11 | lstm_encoder                       | LSTM                            | 7.4 K \n",
      "12 | lstm_decoder                       | LSTM                            | 7.4 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 1.9 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 60    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 4.7 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 2.8 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 1.9 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 3.8 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 1.9 K \n",
      "20 | output_layer                       | Linear                          | 31    \n",
      "----------------------------------------------------------------------------------------\n",
      "220 K     Trainable params\n",
      "0         Non-trainable params\n",
      "220 K     Total params\n",
      "0.881     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 7/7 [00:00<00:00,  8.01it/s, loss=0.112, train_loss_step=0.0876, val_loss=0.155, train_loss_epoch=0.100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:35:03,125]\u001b[0m Trial 22 finished with value: 0.15505506098270416 and parameters: {'batch_size': 8, 'hidden_size': 30, 'attention_head_size': 2, 'dropout': 0.040137047359038225, 'learning_rate': 0.007336423451686361}. Best is trial 16 with value: 0.07138101011514664.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.6 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 114 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 112 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 7.0 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 7.0 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 7.0 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 7.0 K \n",
      "11 | lstm_encoder                       | LSTM                            | 13.8 K\n",
      "12 | lstm_decoder                       | LSTM                            | 13.8 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 3.4 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 82    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 8.7 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 5.0 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 3.5 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 7.0 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 3.5 K \n",
      "20 | output_layer                       | Linear                          | 42    \n",
      "----------------------------------------------------------------------------------------\n",
      "316 K     Trainable params\n",
      "0         Non-trainable params\n",
      "316 K     Total params\n",
      "1.266     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 7/7 [00:00<00:00,  7.75it/s, loss=0.125, train_loss_step=0.119, val_loss=0.0837, train_loss_epoch=0.122]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:35:15,950]\u001b[0m Trial 23 finished with value: 0.08365754038095474 and parameters: {'batch_size': 8, 'hidden_size': 41, 'attention_head_size': 2, 'dropout': 0.25115652458877524, 'learning_rate': 0.0033374075460964815}. Best is trial 16 with value: 0.07138101011514664.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.7 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 153 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 151 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 12.9 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 12.9 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 12.9 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 12.9 K\n",
      "11 | lstm_encoder                       | LSTM                            | 25.5 K\n",
      "12 | lstm_decoder                       | LSTM                            | 25.5 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 6.4 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 112   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 16.0 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 12.7 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 6.5 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 12.9 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 6.5 K \n",
      "20 | output_layer                       | Linear                          | 57    \n",
      "----------------------------------------------------------------------------------------\n",
      "472 K     Trainable params\n",
      "0         Non-trainable params\n",
      "472 K     Total params\n",
      "1.889     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 7/7 [00:00<00:00,  7.50it/s, loss=0.136, train_loss_step=0.102, val_loss=0.107, train_loss_epoch=0.127]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:35:27,404]\u001b[0m Trial 24 finished with value: 0.10663852840662003 and parameters: {'batch_size': 8, 'hidden_size': 56, 'attention_head_size': 1, 'dropout': 0.12563484908238676, 'learning_rate': 0.0018898203885517433}. Best is trial 16 with value: 0.07138101011514664.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.3 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 137 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 135 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 10.3 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 10.3 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 10.3 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 10.3 K\n",
      "11 | lstm_encoder                       | LSTM                            | 20.4 K\n",
      "12 | lstm_decoder                       | LSTM                            | 20.4 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 5.1 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 100   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 12.8 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 6.5 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 5.2 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 10.3 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 5.2 K \n",
      "20 | output_layer                       | Linear                          | 51    \n",
      "----------------------------------------------------------------------------------------\n",
      "404 K     Trainable params\n",
      "0         Non-trainable params\n",
      "404 K     Total params\n",
      "1.616     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 7/7 [00:00<00:00,  7.57it/s, loss=0.168, train_loss_step=0.122, val_loss=0.157, train_loss_epoch=0.146]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:35:39,680]\u001b[0m Trial 25 finished with value: 0.156851664185524 and parameters: {'batch_size': 8, 'hidden_size': 50, 'attention_head_size': 3, 'dropout': 0.2441760617810736, 'learning_rate': 0.004397831851162384}. Best is trial 16 with value: 0.07138101011514664.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.7 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 116 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 115 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 7.3 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 7.3 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 7.3 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 7.3 K \n",
      "11 | lstm_encoder                       | LSTM                            | 14.4 K\n",
      "12 | lstm_decoder                       | LSTM                            | 14.4 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 3.6 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 84    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 9.1 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 5.4 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 3.7 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 7.3 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 3.7 K \n",
      "20 | output_layer                       | Linear                          | 43    \n",
      "----------------------------------------------------------------------------------------\n",
      "325 K     Trainable params\n",
      "0         Non-trainable params\n",
      "325 K     Total params\n",
      "1.304     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 4/4 [00:00<00:00,  7.01it/s, loss=0.539, train_loss_step=0.556, val_loss=0.321, train_loss_epoch=0.508]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:35:43,882]\u001b[0m Trial 26 finished with value: 0.32067248225212097 and parameters: {'batch_size': 16, 'hidden_size': 42, 'attention_head_size': 2, 'dropout': 0.18326463875345583, 'learning_rate': 0.00020847134265345097}. Best is trial 16 with value: 0.07138101011514664.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 1.8 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 51.3 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 50.6 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K \n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576   \n",
      "20 | output_layer                       | Linear                          | 17    \n",
      "----------------------------------------------------------------------------------------\n",
      "116 K     Trainable params\n",
      "0         Non-trainable params\n",
      "116 K     Total params\n",
      "0.467     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 7/7 [00:00<00:00,  9.13it/s, loss=0.11, train_loss_step=0.0972, val_loss=0.0567, train_loss_epoch=0.104] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:35:54,973]\u001b[0m Trial 27 finished with value: 0.056701671332120895 and parameters: {'batch_size': 8, 'hidden_size': 16, 'attention_head_size': 1, 'dropout': 0.10501453459070806, 'learning_rate': 0.007120219150765482}. Best is trial 27 with value: 0.056701671332120895.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 1.7 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 48.8 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 48.1 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 990   \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 990   \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 990   \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 990   \n",
      "11 | lstm_encoder                       | LSTM                            | 1.9 K \n",
      "12 | lstm_decoder                       | LSTM                            | 1.9 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 480   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 30    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.2 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 665   \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 510   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 990   \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 510   \n",
      "20 | output_layer                       | Linear                          | 16    \n",
      "----------------------------------------------------------------------------------------\n",
      "109 K     Trainable params\n",
      "0         Non-trainable params\n",
      "109 K     Total params\n",
      "0.439     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 7/7 [00:00<00:00,  8.40it/s, loss=0.169, train_loss_step=0.133, val_loss=0.119, train_loss_epoch=0.144] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:36:03,209]\u001b[0m Trial 28 finished with value: 0.1191493347287178 and parameters: {'batch_size': 8, 'hidden_size': 15, 'attention_head_size': 2, 'dropout': 0.04387737307713756, 'learning_rate': 0.006517627670110531}. Best is trial 27 with value: 0.056701671332120895.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 1.2 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 29.9 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 29.4 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 304   \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 304   \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 304   \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 304   \n",
      "11 | lstm_encoder                       | LSTM                            | 576   \n",
      "12 | lstm_decoder                       | LSTM                            | 576   \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 144   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 16    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 368   \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 142   \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 160   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 304   \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 160   \n",
      "20 | output_layer                       | Linear                          | 9     \n",
      "----------------------------------------------------------------------------------------\n",
      "63.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "63.0 K    Total params\n",
      "0.252     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 7/7 [00:00<00:00, 12.89it/s, loss=0.666, train_loss_step=0.577, val_loss=0.436, train_loss_epoch=0.661]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 13:36:06,586]\u001b[0m Trial 29 finished with value: 0.4356025755405426 and parameters: {'batch_size': 8, 'hidden_size': 8, 'attention_head_size': 3, 'dropout': 0.09243320597303416, 'learning_rate': 0.0011426498419219344}. Best is trial 27 with value: 0.056701671332120895.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best SMAPE: 0.056701671332120895\n",
      "Best Params:\n",
      "  batch_size: 8\n",
      "  hidden_size: 16\n",
      "  attention_head_size: 1\n",
      "  dropout: 0.10501453459070806\n",
      "  learning_rate: 0.007120219150765482\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", study_name=\"TFT_Petrol_Tuning\")\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "print(\"Best SMAPE:\", study.best_value)\n",
    "print(\"Best Params:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retraining with the best parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.2 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 1.8 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 51.3 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 50.6 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K \n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576   \n",
      "20 | output_layer                       | Linear                          | 17    \n",
      "----------------------------------------------------------------------------------------\n",
      "116 K     Trainable params\n",
      "0         Non-trainable params\n",
      "116 K     Total params\n",
      "0.467     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 7/7 [00:01<00:00,  4.90it/s, loss=0.443, v_num=31, train_loss_step=0.302, val_loss=1.340]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 7/7 [00:01<00:00,  4.55it/s, loss=0.448, v_num=31, train_loss_step=0.190, val_loss=1.210, train_loss_epoch=0.443]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.134 >= min_delta = 0.0. New best score: 1.207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 7/7 [00:01<00:00,  4.38it/s, loss=0.426, v_num=31, train_loss_step=0.438, val_loss=1.000, train_loss_epoch=0.453]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.205 >= min_delta = 0.0. New best score: 1.002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 7/7 [00:01<00:00,  4.84it/s, loss=0.386, v_num=31, train_loss_step=0.323, val_loss=0.784, train_loss_epoch=0.383]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.219 >= min_delta = 0.0. New best score: 0.784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 7/7 [00:01<00:00,  4.75it/s, loss=0.351, v_num=31, train_loss_step=0.332, val_loss=0.634, train_loss_epoch=0.358]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.150 >= min_delta = 0.0. New best score: 0.634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 7/7 [00:01<00:00,  4.66it/s, loss=0.323, v_num=31, train_loss_step=0.218, val_loss=0.441, train_loss_epoch=0.318]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.193 >= min_delta = 0.0. New best score: 0.441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 7/7 [00:01<00:00,  4.77it/s, loss=0.283, v_num=31, train_loss_step=0.220, val_loss=0.284, train_loss_epoch=0.265]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.157 >= min_delta = 0.0. New best score: 0.284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 7/7 [00:01<00:00,  4.78it/s, loss=0.249, v_num=31, train_loss_step=0.195, val_loss=0.171, train_loss_epoch=0.239]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.113 >= min_delta = 0.0. New best score: 0.171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 7/7 [00:01<00:00,  4.68it/s, loss=0.214, v_num=31, train_loss_step=0.172, val_loss=0.0967, train_loss_epoch=0.213]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.074 >= min_delta = 0.0. New best score: 0.097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 7/7 [00:01<00:00,  4.82it/s, loss=0.193, v_num=31, train_loss_step=0.163, val_loss=0.0753, train_loss_epoch=0.182]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.021 >= min_delta = 0.0. New best score: 0.075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 7/7 [00:01<00:00,  4.03it/s, loss=0.17, v_num=31, train_loss_step=0.137, val_loss=0.075, train_loss_epoch=0.170]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 7/7 [00:01<00:00,  4.66it/s, loss=0.155, v_num=31, train_loss_step=0.134, val_loss=0.0748, train_loss_epoch=0.149]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 7/7 [00:01<00:00,  4.82it/s, loss=0.141, v_num=31, train_loss_step=0.111, val_loss=0.0746, train_loss_epoch=0.141]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 7/7 [00:01<00:00,  4.84it/s, loss=0.112, v_num=31, train_loss_step=0.0944, val_loss=0.0745, train_loss_epoch=0.109]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 7/7 [00:01<00:00,  4.79it/s, loss=0.105, v_num=31, train_loss_step=0.0901, val_loss=0.0743, train_loss_epoch=0.107]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 7/7 [00:01<00:00,  4.80it/s, loss=0.0991, v_num=31, train_loss_step=0.0813, val_loss=0.0742, train_loss_epoch=0.095]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 7/7 [00:01<00:00,  4.85it/s, loss=0.0635, v_num=31, train_loss_step=0.0584, val_loss=0.0857, train_loss_epoch=0.0636]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 10 records. Best score: 0.074. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 7/7 [00:02<00:00,  3.40it/s, loss=0.0635, v_num=31, train_loss_step=0.0584, val_loss=0.0857, train_loss_epoch=0.0613]\n"
     ]
    }
   ],
   "source": [
    "train_loader = training.to_dataloader(train=True, batch_size=8, num_workers=0)\n",
    "val_loader = validation.to_dataloader(train=False, batch_size=8, num_workers=0)\n",
    "\n",
    "# Define TFT with Optuna-optimized parameters\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.10501453459070806,\n",
    "    learning_rate=0.007120219150765482,\n",
    "    loss=SMAPE(),\n",
    "    log_interval=10,\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "# Lightning Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, verbose=True, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"cpu\",\n",
    "    callbacks=[early_stop, lr_logger],\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(tft, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Final Evaluation Metrics (Optuna-tuned TFT):\n",
      "SMAPE: 0.09\n",
      "MAE: 34.39\n",
      "RMSE: 34.39\n",
      "R² Score: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/sklearn/metrics/_regression.py:796: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "best_model = TemporalFusionTransformer.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "actuals = torch.cat([y[0] for x, y in iter(val_loader)])\n",
    "predictions = best_model.predict(val_loader)\n",
    "\n",
    "y_true = actuals.detach().cpu().numpy()\n",
    "y_pred = predictions.detach().cpu().numpy()\n",
    "\n",
    "print(f\"\\n🔍 Final Evaluation Metrics (Optuna-tuned TFT):\")\n",
    "print(f\"SMAPE: {SMAPE()(predictions, actuals).item():.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_true, y_pred):.2f}\")\n",
    "print(f\"RMSE: {mean_squared_error(y_true, y_pred, squared=False):.2f}\")\n",
    "print(f\"R² Score: {r2_score(y_true, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training TFT for petrol price prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/yehana2002/Projects/DSGP/datasets/processed/final_merged_dataset_ready.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df = df.sort_values(\"Date\").reset_index(drop=True)\n",
    "df[\"series_id\"] = \"sri_lanka\"\n",
    "df.rename(columns={\"Petrol_Price\": \"petrol_price\"}, inplace=True)\n",
    "df[\"time_idx\"] = np.arange(len(df))\n",
    "\n",
    "df.fillna(method=\"ffill\", inplace=True)\n",
    "df.fillna(method=\"bfill\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"crude_roc\"] = df[\"Crude_Oil_Price\"].pct_change().fillna(0)\n",
    "df[\"usd_lkr_roc\"] = df[\"USD_LKR\"].pct_change().fillna(0)\n",
    "\n",
    "# 3-month lag\n",
    "df[\"petrol_lag3\"] = df[\"petrol_price\"].shift(3).fillna(method=\"bfill\")\n",
    "df[\"crude_lag3\"] = df[\"Crude_Oil_Price\"].shift(3).fillna(method=\"bfill\")\n",
    "\n",
    "# Interaction terms\n",
    "df[\"crude_usd_interaction\"] = df[\"Crude_Oil_Price\"] * df[\"USD_LKR\"]\n",
    "df[\"inflation_x_policy\"] = df[\"annual_cpi_inflation_rate\"] * df[\"policy_rate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFT configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_length = 36\n",
    "max_prediction_length = 12\n",
    "training_cutoff = df[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "time_varying_known_reals = [\n",
    "    \"time_idx\", \"Date\"\n",
    "] + [col for col in df.columns if col not in [\n",
    "    \"petrol_price\", \"diesel_price\", \"series_id\", \"time_idx\"\n",
    "] and df[col].dtype in [np.float64, np.int64]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time series dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cutoff = df[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "tft_training = TimeSeriesDataSet(\n",
    "    df[df.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"petrol_price\",\n",
    "    group_ids=[\"series_id\"],\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"series_id\"],\n",
    "    time_varying_known_reals=time_varying_known_reals,\n",
    "    time_varying_unknown_reals=[\"petrol_price\"],\n",
    "    target_normalizer=GroupNormalizer(groups=[\"series_id\"]),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "tft_validation = TimeSeriesDataSet.from_dataset(tft_training, df, predict=True, stop_randomization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_dataloader = tft_training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = tft_validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    tft_training,\n",
    "    learning_rate=0.007120219150765482,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.10501453459070806,\n",
    "    loss=SMAPE(),\n",
    "    log_interval=10,\n",
    "    reduce_on_plateau_patience=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=10, verbose=True, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=100,\n",
    "    accelerator=\"cpu\",\n",
    "    callbacks=[early_stop_callback, lr_logger],\n",
    "    gradient_clip_val=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 1.8 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 55.4 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 54.8 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K \n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576   \n",
      "20 | output_layer                       | Linear                          | 17    \n",
      "----------------------------------------------------------------------------------------\n",
      "124 K     Trainable params\n",
      "0         Non-trainable params\n",
      "124 K     Total params\n",
      "0.500     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  9.68it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 7/7 [00:01<00:00,  4.51it/s, loss=0.518, v_num=33, train_loss_step=0.445, val_loss=2.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 2.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 7/7 [00:01<00:00,  3.87it/s, loss=0.46, v_num=33, train_loss_step=0.372, val_loss=1.740, train_loss_epoch=0.480] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.257 >= min_delta = 0.0. New best score: 1.743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 7/7 [00:01<00:00,  4.65it/s, loss=0.393, v_num=33, train_loss_step=0.319, val_loss=1.180, train_loss_epoch=0.355]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.563 >= min_delta = 0.0. New best score: 1.180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 7/7 [00:01<00:00,  4.52it/s, loss=0.309, v_num=33, train_loss_step=0.272, val_loss=0.789, train_loss_epoch=0.305]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.391 >= min_delta = 0.0. New best score: 0.789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 7/7 [00:01<00:00,  4.33it/s, loss=0.263, v_num=33, train_loss_step=0.169, val_loss=0.503, train_loss_epoch=0.252]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.286 >= min_delta = 0.0. New best score: 0.503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 7/7 [00:01<00:00,  4.53it/s, loss=0.223, v_num=33, train_loss_step=0.163, val_loss=0.281, train_loss_epoch=0.215]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.222 >= min_delta = 0.0. New best score: 0.281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 7/7 [00:01<00:00,  4.59it/s, loss=0.195, v_num=33, train_loss_step=0.188, val_loss=0.167, train_loss_epoch=0.188]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.114 >= min_delta = 0.0. New best score: 0.167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 7/7 [00:01<00:00,  4.61it/s, loss=0.168, v_num=33, train_loss_step=0.124, val_loss=0.103, train_loss_epoch=0.165]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.064 >= min_delta = 0.0. New best score: 0.103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 7/7 [00:01<00:00,  4.61it/s, loss=0.151, v_num=33, train_loss_step=0.131, val_loss=0.0623, train_loss_epoch=0.149]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.041 >= min_delta = 0.0. New best score: 0.062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 7/7 [00:01<00:00,  4.62it/s, loss=0.137, v_num=33, train_loss_step=0.101, val_loss=0.060, train_loss_epoch=0.134] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 7/7 [00:01<00:00,  4.31it/s, loss=0.0785, v_num=33, train_loss_step=0.0758, val_loss=0.0645, train_loss_epoch=0.0796]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 10 records. Best score: 0.060. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 7/7 [00:02<00:00,  3.01it/s, loss=0.0785, v_num=33, train_loss_step=0.0758, val_loss=0.0645, train_loss_epoch=0.075] \n"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "best_model = TemporalFusionTransformer.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "predictions = best_model.predict(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = actuals.detach().cpu().numpy()\n",
    "y_pred = predictions.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Evaluation Metrics for Petrol Price:\n",
      "SMAPE: 0.06\n",
      "MAE: 26.04\n",
      "RMSE: 26.04\n",
      "R² Score: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/sklearn/metrics/_regression.py:796: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "smape = SMAPE()(predictions, actuals).item()\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(f\"\\n🔍 Evaluation Metrics for Petrol Price:\")\n",
    "print(f\"SMAPE: {smape:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = actuals - predictions  # shape: (num_batches, prediction_length)\n",
    "residuals_np = residuals.detach().cpu().numpy().flatten()\n",
    "\n",
    "# Extracting dates from x object used in prediction\n",
    "# Each sample in x has 'decoder_time_idx' which maps to actual prediction dates\n",
    "decoded_time = x[\"decoder_time_idx\"].detach().cpu().numpy().flatten()\n",
    "\n",
    "# Match with dates from the original dataframe\n",
    "date_lookup = df.set_index(\"time_idx\")[\"Date\"]\n",
    "dates = pd.to_datetime([date_lookup[int(t)] for t in decoded_time])\n",
    "\n",
    "# Now build residual DataFrame\n",
    "res_df = pd.DataFrame({\n",
    "    \"date\": dates,\n",
    "    \"residuals\": residuals_np\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.to_csv(\"tft_petrol_residuals.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training TFT for diesel price prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "agri_index_df = pd.read_csv(\"/Users/yehana2002/Projects/DSGP/datasets/raw/diesel_other_factros.csv\")\n",
    "agri_index_df = agri_index_df[[\"Date\", \"sl_agri_production_index\"]]\n",
    "agri_index_df[\"Date\"] = pd.to_datetime(agri_index_df[\"Date\"])\n",
    "agri_index_df[\"Year\"] = agri_index_df[\"Date\"].dt.year\n",
    "agri_index_df = agri_index_df[[\"Year\", \"sl_agri_production_index\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_csv(\"/Users/yehana2002/Projects/DSGP/datasets/processed/final_merged_dataset_ready.csv\")\n",
    "final_df[\"Date\"] = pd.to_datetime(final_df[\"Date\"])\n",
    "final_df[\"Year\"] = final_df[\"Date\"].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.merge(agri_index_df, on=\"Year\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"sl_agri_production_index\"].fillna(method=\"ffill\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Diesel_Price</th>\n",
       "      <th>Year</th>\n",
       "      <th>sl_agri_production_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1992-08-21</td>\n",
       "      <td>14.5</td>\n",
       "      <td>1992</td>\n",
       "      <td>52.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1992-12-21</td>\n",
       "      <td>14.5</td>\n",
       "      <td>1992</td>\n",
       "      <td>52.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1993-06-07</td>\n",
       "      <td>14.5</td>\n",
       "      <td>1993</td>\n",
       "      <td>61.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1993-06-11</td>\n",
       "      <td>14.5</td>\n",
       "      <td>1993</td>\n",
       "      <td>61.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1993-07-12</td>\n",
       "      <td>14.6</td>\n",
       "      <td>1993</td>\n",
       "      <td>61.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Diesel_Price  Year  sl_agri_production_index\n",
       "0 1992-08-21          14.5  1992                     52.99\n",
       "1 1992-12-21          14.5  1992                     52.99\n",
       "2 1993-06-07          14.5  1993                     61.51\n",
       "3 1993-06-11          14.5  1993                     61.51\n",
       "4 1993-07-12          14.6  1993                     61.51"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df[[\"Date\", \"Diesel_Price\", \"Year\", \"sl_agri_production_index\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(\"Date\").reset_index(drop=True)\n",
    "df.rename(columns={\"Diesel_Price\": \"diesel_price\"}, inplace=True)\n",
    "df[\"series_id\"] = \"sri_lanka\"\n",
    "df[\"time_idx\"] = np.arange(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_length = 36\n",
    "max_prediction_length = 12\n",
    "training_cutoff = df[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "time_varying_known_reals = [\n",
    "    \"time_idx\", \"Date\"\n",
    "] + [col for col in df.columns if col not in [\"diesel_price\", \"petrol_price\", \"series_id\", \"time_idx\"] and df[col].dtype in [np.float64, np.int64]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = TimeSeriesDataSet(\n",
    "    df[df.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"diesel_price\",\n",
    "    group_ids=[\"series_id\"],\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"series_id\"],\n",
    "    time_varying_known_reals=time_varying_known_reals,\n",
    "    time_varying_unknown_reals=[\"diesel_price\"],\n",
    "    target_normalizer=GroupNormalizer(groups=[\"series_id\"]),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(training, df, predict=True, stop_randomization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optuna testing for diesel model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 8, 64)\n",
    "    attention_head_size = trial.suggest_int(\"attention_head_size\", 1, 4)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.3)\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)\n",
    "\n",
    "    train_loader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "    val_loader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    model = TemporalFusionTransformer.from_dataset(\n",
    "        training,\n",
    "        hidden_size=hidden_size,\n",
    "        attention_head_size=attention_head_size,\n",
    "        dropout=dropout,\n",
    "        learning_rate=learning_rate,\n",
    "        loss=SMAPE(),\n",
    "        log_interval=0,\n",
    "        reduce_on_plateau_patience=4,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        max_epochs=20,\n",
    "        accelerator=\"cpu\",\n",
    "        logger=False,\n",
    "        enable_checkpointing=False,\n",
    "        callbacks=[EarlyStopping(monitor=\"val_loss\", patience=5)],\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "    val_actuals = torch.cat([y[0] for x, y in iter(val_loader)])\n",
    "    val_predictions = model.predict(val_loader)\n",
    "\n",
    "    smape_score = SMAPE()(val_predictions, val_actuals).item()\n",
    "    return smape_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 16:56:04,565]\u001b[0m A new study created in memory with name: TFT_Diesel_Tuning\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 1.2 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 32.3 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 31.9 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 304   \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 304   \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 304   \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 304   \n",
      "11 | lstm_encoder                       | LSTM                            | 576   \n",
      "12 | lstm_decoder                       | LSTM                            | 576   \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 144   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 16    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 368   \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 178   \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 160   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 304   \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 160   \n",
      "20 | output_layer                       | Linear                          | 9     \n",
      "----------------------------------------------------------------------------------------\n",
      "67.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "67.8 K    Total params\n",
      "0.271     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00, 11.91it/s, loss=0.451, train_loss_step=0.406, val_loss=1.100, train_loss_epoch=0.406]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00, 11.84it/s, loss=0.451, train_loss_step=0.406, val_loss=1.100, train_loss_epoch=0.406]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 16:56:08,406]\u001b[0m Trial 0 finished with value: 1.0984548330307007 and parameters: {'batch_size': 32, 'hidden_size': 8, 'attention_head_size': 4, 'dropout': 0.13922589728412424, 'learning_rate': 0.001274811529330767}. Best is trial 0 with value: 1.0984548330307007.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 5.1 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 182 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 180 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 15.7 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 15.7 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 15.7 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 15.7 K\n",
      "11 | lstm_encoder                       | LSTM                            | 31.2 K\n",
      "12 | lstm_decoder                       | LSTM                            | 31.2 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 7.8 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 124   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 19.6 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 11.7 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 7.9 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 15.7 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 7.9 K \n",
      "20 | output_layer                       | Linear                          | 63    \n",
      "----------------------------------------------------------------------------------------\n",
      "563 K     Trainable params\n",
      "0         Non-trainable params\n",
      "563 K     Total params\n",
      "2.253     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 4/4 [00:00<00:00,  5.53it/s, loss=0.195, train_loss_step=0.167, val_loss=0.176, train_loss_epoch=0.171] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 16:56:20,569]\u001b[0m Trial 1 finished with value: 0.17587685585021973 and parameters: {'batch_size': 16, 'hidden_size': 62, 'attention_head_size': 2, 'dropout': 0.05069085243340647, 'learning_rate': 0.004701295505814713}. Best is trial 1 with value: 0.17587685585021973.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 2.6 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 85.1 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 84.0 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 3.1 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 3.1 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 3.1 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 3.1 K \n",
      "11 | lstm_encoder                       | LSTM                            | 6.0 K \n",
      "12 | lstm_decoder                       | LSTM                            | 6.0 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 1.5 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 54    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 3.8 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 2.2 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 1.6 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 3.1 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 1.6 K \n",
      "20 | output_layer                       | Linear                          | 28    \n",
      "----------------------------------------------------------------------------------------\n",
      "208 K     Trainable params\n",
      "0         Non-trainable params\n",
      "208 K     Total params\n",
      "0.835     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00,  7.26it/s, loss=0.542, train_loss_step=0.425, val_loss=0.544, train_loss_epoch=0.425]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00,  7.23it/s, loss=0.542, train_loss_step=0.425, val_loss=0.544, train_loss_epoch=0.425]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 16:56:26,332]\u001b[0m Trial 2 finished with value: 0.5440833568572998 and parameters: {'batch_size': 32, 'hidden_size': 27, 'attention_head_size': 2, 'dropout': 0.0005682560601292574, 'learning_rate': 0.0030959541833530282}. Best is trial 1 with value: 0.17587685585021973.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.9 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 134 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 132 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 8.4 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 8.4 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 8.4 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 8.4 K \n",
      "11 | lstm_encoder                       | LSTM                            | 16.6 K\n",
      "12 | lstm_decoder                       | LSTM                            | 16.6 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 4.1 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 90    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 10.4 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 8.2 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 4.2 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 8.4 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 4.2 K \n",
      "20 | output_layer                       | Linear                          | 46    \n",
      "----------------------------------------------------------------------------------------\n",
      "376 K     Trainable params\n",
      "0         Non-trainable params\n",
      "376 K     Total params\n",
      "1.507     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 4/4 [00:00<00:00,  6.28it/s, loss=0.389, train_loss_step=0.355, val_loss=0.622, train_loss_epoch=0.336]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 4/4 [00:00<00:00,  6.27it/s, loss=0.389, train_loss_step=0.355, val_loss=0.622, train_loss_epoch=0.336]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 16:56:39,135]\u001b[0m Trial 3 finished with value: 0.6217017769813538 and parameters: {'batch_size': 16, 'hidden_size': 45, 'attention_head_size': 1, 'dropout': 0.08667813933926133, 'learning_rate': 0.0001388958795440624}. Best is trial 1 with value: 0.17587685585021973.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 5.2 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 185 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 183 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 16.3 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 16.3 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 16.3 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 16.3 K\n",
      "11 | lstm_encoder                       | LSTM                            | 32.3 K\n",
      "12 | lstm_decoder                       | LSTM                            | 32.3 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 8.1 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 126   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 20.2 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 10.7 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 8.2 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 16.3 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 8.2 K \n",
      "20 | output_layer                       | Linear                          | 64    \n",
      "----------------------------------------------------------------------------------------\n",
      "574 K     Trainable params\n",
      "0         Non-trainable params\n",
      "574 K     Total params\n",
      "2.296     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 4/4 [00:00<00:00,  5.62it/s, loss=0.0986, train_loss_step=0.0815, val_loss=0.0835, train_loss_epoch=0.0767]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 4/4 [00:00<00:00,  5.61it/s, loss=0.0986, train_loss_step=0.0815, val_loss=0.0835, train_loss_epoch=0.0767]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 16:56:53,499]\u001b[0m Trial 4 finished with value: 0.08354290574789047 and parameters: {'batch_size': 16, 'hidden_size': 63, 'attention_head_size': 3, 'dropout': 5.71315030161168e-05, 'learning_rate': 0.0013366258010220143}. Best is trial 4 with value: 0.08354290574789047.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.4 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 115 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 113 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 6.0 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 6.0 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 6.0 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 6.0 K \n",
      "11 | lstm_encoder                       | LSTM                            | 11.9 K\n",
      "12 | lstm_decoder                       | LSTM                            | 11.9 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 3.0 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 76    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 7.4 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 5.9 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 3.0 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 6.0 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 3.0 K \n",
      "20 | output_layer                       | Linear                          | 39    \n",
      "----------------------------------------------------------------------------------------\n",
      "307 K     Trainable params\n",
      "0         Non-trainable params\n",
      "307 K     Total params\n",
      "1.230     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00,  7.54it/s, loss=0.396, train_loss_step=0.354, val_loss=0.402, train_loss_epoch=0.389]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00,  7.53it/s, loss=0.396, train_loss_step=0.354, val_loss=0.402, train_loss_epoch=0.389]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 16:57:12,306]\u001b[0m Trial 5 finished with value: 0.40248024463653564 and parameters: {'batch_size': 8, 'hidden_size': 38, 'attention_head_size': 1, 'dropout': 0.19012744622237504, 'learning_rate': 0.0006248849720219954}. Best is trial 4 with value: 0.08354290574789047.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.8 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 168 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 166 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 13.3 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 13.3 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 13.3 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 13.3 K\n",
      "11 | lstm_encoder                       | LSTM                            | 26.4 K\n",
      "12 | lstm_decoder                       | LSTM                            | 26.4 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 6.6 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 114   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 16.6 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 13.2 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 6.7 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 13.3 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 6.7 K \n",
      "20 | output_layer                       | Linear                          | 58    \n",
      "----------------------------------------------------------------------------------------\n",
      "507 K     Trainable params\n",
      "0         Non-trainable params\n",
      "507 K     Total params\n",
      "2.031     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00,  5.94it/s, loss=0.575, train_loss_step=0.474, val_loss=0.415, train_loss_epoch=0.474]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00,  5.92it/s, loss=0.575, train_loss_step=0.474, val_loss=0.415, train_loss_epoch=0.474]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 16:57:19,397]\u001b[0m Trial 6 finished with value: 0.41468545794487 and parameters: {'batch_size': 32, 'hidden_size': 57, 'attention_head_size': 1, 'dropout': 0.03768522190546938, 'learning_rate': 0.00016014575553082125}. Best is trial 4 with value: 0.08354290574789047.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.4 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 154 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 152 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 11.1 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 11.1 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 11.1 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 11.1 K\n",
      "11 | lstm_encoder                       | LSTM                            | 22.0 K\n",
      "12 | lstm_decoder                       | LSTM                            | 22.0 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 5.5 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 104   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 13.8 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 6.9 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 5.6 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 11.1 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 5.6 K \n",
      "20 | output_layer                       | Linear                          | 53    \n",
      "----------------------------------------------------------------------------------------\n",
      "447 K     Trainable params\n",
      "0         Non-trainable params\n",
      "447 K     Total params\n",
      "1.789     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00,  7.12it/s, loss=0.283, train_loss_step=0.277, val_loss=1.270, train_loss_epoch=0.269]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00,  7.11it/s, loss=0.283, train_loss_step=0.277, val_loss=1.270, train_loss_epoch=0.269]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 16:57:40,716]\u001b[0m Trial 7 finished with value: 1.2692643404006958 and parameters: {'batch_size': 8, 'hidden_size': 52, 'attention_head_size': 4, 'dropout': 0.1312595433756385, 'learning_rate': 0.00011204180922231109}. Best is trial 4 with value: 0.08354290574789047.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.0 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 137 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 135 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 8.7 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 8.7 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 8.7 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 8.7 K \n",
      "11 | lstm_encoder                       | LSTM                            | 17.3 K\n",
      "12 | lstm_decoder                       | LSTM                            | 17.3 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 4.3 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 92    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 10.9 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 5.6 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 4.4 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 8.7 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 4.4 K \n",
      "20 | output_layer                       | Linear                          | 47    \n",
      "----------------------------------------------------------------------------------------\n",
      "383 K     Trainable params\n",
      "0         Non-trainable params\n",
      "383 K     Total params\n",
      "1.536     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 7/7 [00:00<00:00,  7.11it/s, loss=0.63, train_loss_step=0.580, val_loss=0.476, train_loss_epoch=0.602] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 16:57:51,797]\u001b[0m Trial 8 finished with value: 0.47556909918785095 and parameters: {'batch_size': 8, 'hidden_size': 46, 'attention_head_size': 3, 'dropout': 0.2662527706205085, 'learning_rate': 0.00014445576899354835}. Best is trial 4 with value: 0.08354290574789047.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.5 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 157 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 155 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 11.6 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 11.6 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 11.6 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 11.6 K\n",
      "11 | lstm_encoder                       | LSTM                            | 22.9 K\n",
      "12 | lstm_decoder                       | LSTM                            | 22.9 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 5.7 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 106   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 14.4 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 7.3 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 5.8 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 11.6 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 5.8 K \n",
      "20 | output_layer                       | Linear                          | 54    \n",
      "----------------------------------------------------------------------------------------\n",
      "458 K     Trainable params\n",
      "0         Non-trainable params\n",
      "458 K     Total params\n",
      "1.833     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00,  5.70it/s, loss=0.359, train_loss_step=0.206, val_loss=0.138, train_loss_epoch=0.206] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00,  5.68it/s, loss=0.359, train_loss_step=0.206, val_loss=0.138, train_loss_epoch=0.206]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 16:57:58,653]\u001b[0m Trial 9 finished with value: 0.13784198462963104 and parameters: {'batch_size': 32, 'hidden_size': 53, 'attention_head_size': 3, 'dropout': 0.21157128374499476, 'learning_rate': 0.007570671575891093}. Best is trial 4 with value: 0.08354290574789047.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 2.3 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 74.3 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 73.3 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 2.3 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 2.3 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 2.3 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 2.3 K \n",
      "11 | lstm_encoder                       | LSTM                            | 4.4 K \n",
      "12 | lstm_decoder                       | LSTM                            | 4.4 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 1.1 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 46    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 2.8 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.3 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 1.2 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 2.3 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 1.2 K \n",
      "20 | output_layer                       | Linear                          | 24    \n",
      "----------------------------------------------------------------------------------------\n",
      "176 K     Trainable params\n",
      "0         Non-trainable params\n",
      "176 K     Total params\n",
      "0.705     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 4/4 [00:00<00:00,  7.16it/s, loss=0.563, train_loss_step=0.591, val_loss=0.518, train_loss_epoch=0.538]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 4/4 [00:00<00:00,  7.15it/s, loss=0.563, train_loss_step=0.591, val_loss=0.518, train_loss_epoch=0.538]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 16:58:10,312]\u001b[0m Trial 10 finished with value: 0.5179170966148376 and parameters: {'batch_size': 16, 'hidden_size': 23, 'attention_head_size': 3, 'dropout': 0.28172925321432674, 'learning_rate': 0.0006073412924502034}. Best is trial 4 with value: 0.08354290574789047.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 5.3 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 188 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 186 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 16.8 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 16.8 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 16.8 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 16.8 K\n",
      "11 | lstm_encoder                       | LSTM                            | 33.3 K\n",
      "12 | lstm_decoder                       | LSTM                            | 33.3 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 8.3 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 128   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 20.9 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 10.9 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 8.4 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 16.8 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 8.4 K \n",
      "20 | output_layer                       | Linear                          | 65    \n",
      "----------------------------------------------------------------------------------------\n",
      "586 K     Trainable params\n",
      "0         Non-trainable params\n",
      "586 K     Total params\n",
      "2.344     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00,  5.86it/s, loss=0.472, train_loss_step=0.237, val_loss=0.0897, train_loss_epoch=0.237]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00,  5.84it/s, loss=0.472, train_loss_step=0.237, val_loss=0.0897, train_loss_epoch=0.237]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 16:58:17,453]\u001b[0m Trial 11 finished with value: 0.08968407660722733 and parameters: {'batch_size': 32, 'hidden_size': 64, 'attention_head_size': 3, 'dropout': 0.20858772419939392, 'learning_rate': 0.0091742835249257}. Best is trial 4 with value: 0.08354290574789047.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 5.3 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 188 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 186 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 16.8 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 16.8 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 16.8 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 16.8 K\n",
      "11 | lstm_encoder                       | LSTM                            | 33.3 K\n",
      "12 | lstm_decoder                       | LSTM                            | 33.3 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 8.3 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 128   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 20.9 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 10.9 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 8.4 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 16.8 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 8.4 K \n",
      "20 | output_layer                       | Linear                          | 65    \n",
      "----------------------------------------------------------------------------------------\n",
      "586 K     Trainable params\n",
      "0         Non-trainable params\n",
      "586 K     Total params\n",
      "2.344     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 4/4 [00:00<00:00,  5.73it/s, loss=0.183, train_loss_step=0.177, val_loss=0.0953, train_loss_epoch=0.155]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 4/4 [00:00<00:00,  5.72it/s, loss=0.183, train_loss_step=0.177, val_loss=0.0953, train_loss_epoch=0.155]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 16:58:31,699]\u001b[0m Trial 12 finished with value: 0.0952734425663948 and parameters: {'batch_size': 16, 'hidden_size': 64, 'attention_head_size': 3, 'dropout': 0.2166873276327294, 'learning_rate': 0.0021695796156208113}. Best is trial 4 with value: 0.08354290574789047.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 5.2 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 185 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 183 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 16.3 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 16.3 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 16.3 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 16.3 K\n",
      "11 | lstm_encoder                       | LSTM                            | 32.3 K\n",
      "12 | lstm_decoder                       | LSTM                            | 32.3 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 8.1 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 126   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 20.2 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 9.6 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 8.2 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 16.3 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 8.2 K \n",
      "20 | output_layer                       | Linear                          | 64    \n",
      "----------------------------------------------------------------------------------------\n",
      "572 K     Trainable params\n",
      "0         Non-trainable params\n",
      "572 K     Total params\n",
      "2.292     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2/2 [00:00<00:00,  5.53it/s, loss=0.595, train_loss_step=0.507, val_loss=0.311, train_loss_epoch=0.507]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 16:58:35,543]\u001b[0m Trial 13 finished with value: 0.3109791576862335 and parameters: {'batch_size': 32, 'hidden_size': 63, 'attention_head_size': 4, 'dropout': 0.17225631819968423, 'learning_rate': 0.008723519492691676}. Best is trial 4 with value: 0.08354290574789047.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.9 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 134 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 132 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 8.4 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 8.4 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 8.4 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 8.4 K \n",
      "11 | lstm_encoder                       | LSTM                            | 16.6 K\n",
      "12 | lstm_decoder                       | LSTM                            | 16.6 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 4.1 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 90    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 10.4 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 6.0 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 4.2 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 8.4 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 4.2 K \n",
      "20 | output_layer                       | Linear                          | 46    \n",
      "----------------------------------------------------------------------------------------\n",
      "374 K     Trainable params\n",
      "0         Non-trainable params\n",
      "374 K     Total params\n",
      "1.498     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 4/4 [00:00<00:00,  6.15it/s, loss=0.353, train_loss_step=0.307, val_loss=0.267, train_loss_epoch=0.310]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 4/4 [00:00<00:00,  6.14it/s, loss=0.353, train_loss_step=0.307, val_loss=0.267, train_loss_epoch=0.310]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 16:58:49,356]\u001b[0m Trial 14 finished with value: 0.26667460799217224 and parameters: {'batch_size': 16, 'hidden_size': 45, 'attention_head_size': 2, 'dropout': 0.24194640851903765, 'learning_rate': 0.0014152497223972594}. Best is trial 4 with value: 0.08354290574789047.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.1 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 104 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 102 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 4.8 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.8 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.8 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 4.8 K \n",
      "11 | lstm_encoder                       | LSTM                            | 9.5 K \n",
      "12 | lstm_decoder                       | LSTM                            | 9.5 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.4 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 68    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 6.0 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.1 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.4 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 4.8 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.4 K \n",
      "20 | output_layer                       | Linear                          | 35    \n",
      "----------------------------------------------------------------------------------------\n",
      "268 K     Trainable params\n",
      "0         Non-trainable params\n",
      "268 K     Total params\n",
      "1.074     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 4/4 [00:00<00:00,  6.70it/s, loss=0.512, train_loss_step=0.404, val_loss=0.679, train_loss_epoch=0.478]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 4/4 [00:00<00:00,  6.69it/s, loss=0.512, train_loss_step=0.404, val_loss=0.679, train_loss_epoch=0.478]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 16:59:01,561]\u001b[0m Trial 15 finished with value: 0.678747832775116 and parameters: {'batch_size': 16, 'hidden_size': 34, 'attention_head_size': 3, 'dropout': 0.103191942878087, 'learning_rate': 0.00038377036896848}. Best is trial 4 with value: 0.08354290574789047.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.6 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 159 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 157 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 12.0 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 12.0 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 12.0 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 12.0 K\n",
      "11 | lstm_encoder                       | LSTM                            | 23.8 K\n",
      "12 | lstm_decoder                       | LSTM                            | 23.8 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 5.9 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 108   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 14.9 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 8.9 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 6.0 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 12.0 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 6.0 K \n",
      "20 | output_layer                       | Linear                          | 55    \n",
      "----------------------------------------------------------------------------------------\n",
      "470 K     Trainable params\n",
      "0         Non-trainable params\n",
      "470 K     Total params\n",
      "1.882     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00,  5.96it/s, loss=0.482, train_loss_step=0.228, val_loss=0.151, train_loss_epoch=0.228]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00,  5.94it/s, loss=0.482, train_loss_step=0.228, val_loss=0.151, train_loss_epoch=0.228]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 16:59:08,737]\u001b[0m Trial 16 finished with value: 0.15063147246837616 and parameters: {'batch_size': 32, 'hidden_size': 54, 'attention_head_size': 2, 'dropout': 0.006193514895418838, 'learning_rate': 0.0040699599013324095}. Best is trial 4 with value: 0.08354290574789047.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 1.3 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 36.8 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 36.4 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 378   \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 378   \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 378   \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 378   \n",
      "11 | lstm_encoder                       | LSTM                            | 720   \n",
      "12 | lstm_decoder                       | LSTM                            | 720   \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 180   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 18    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 459   \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 198   \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 198   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 378   \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 198   \n",
      "20 | output_layer                       | Linear                          | 10    \n",
      "----------------------------------------------------------------------------------------\n",
      "77.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "77.9 K    Total params\n",
      "0.311     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 4/4 [00:00<00:00,  8.10it/s, loss=0.682, train_loss_step=0.568, val_loss=0.782, train_loss_epoch=0.646]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 4/4 [00:00<00:00,  8.08it/s, loss=0.682, train_loss_step=0.568, val_loss=0.782, train_loss_epoch=0.646]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 16:59:19,145]\u001b[0m Trial 17 finished with value: 0.7820293307304382 and parameters: {'batch_size': 16, 'hidden_size': 9, 'attention_head_size': 4, 'dropout': 0.1660525270106718, 'learning_rate': 0.00029830333745294014}. Best is trial 4 with value: 0.08354290574789047.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.9 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 174 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 171 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 14.3 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 14.3 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 14.3 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 14.3 K\n",
      "11 | lstm_encoder                       | LSTM                            | 28.3 K\n",
      "12 | lstm_decoder                       | LSTM                            | 28.3 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 7.1 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 118   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 17.8 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 9.1 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 7.2 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 14.3 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 7.2 K \n",
      "20 | output_layer                       | Linear                          | 60    \n",
      "----------------------------------------------------------------------------------------\n",
      "526 K     Trainable params\n",
      "0         Non-trainable params\n",
      "526 K     Total params\n",
      "2.105     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 27.99it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00,  5.61it/s, loss=0.391, train_loss_step=0.267, val_loss=0.444, train_loss_epoch=0.267]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00,  5.60it/s, loss=0.391, train_loss_step=0.267, val_loss=0.444, train_loss_epoch=0.267]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 16:59:26,433]\u001b[0m Trial 18 finished with value: 0.443815678358078 and parameters: {'batch_size': 32, 'hidden_size': 59, 'attention_head_size': 3, 'dropout': 0.08568620738723899, 'learning_rate': 0.002051828067165024}. Best is trial 4 with value: 0.08354290574789047.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.4 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 115 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 113 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 6.0 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 6.0 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 6.0 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 6.0 K \n",
      "11 | lstm_encoder                       | LSTM                            | 11.9 K\n",
      "12 | lstm_decoder                       | LSTM                            | 11.9 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 3.0 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 76    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 7.4 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 4.4 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 3.0 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 6.0 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 3.0 K \n",
      "20 | output_layer                       | Linear                          | 39    \n",
      "----------------------------------------------------------------------------------------\n",
      "305 K     Trainable params\n",
      "0         Non-trainable params\n",
      "305 K     Total params\n",
      "1.224     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 7/7 [00:00<00:00,  7.39it/s, loss=0.279, train_loss_step=0.243, val_loss=0.158, train_loss_epoch=0.261] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 16:59:41,956]\u001b[0m Trial 19 finished with value: 0.15750490128993988 and parameters: {'batch_size': 8, 'hidden_size': 38, 'attention_head_size': 2, 'dropout': 0.24927212107122426, 'learning_rate': 0.0008349311204576643}. Best is trial 4 with value: 0.08354290574789047.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.3 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 148 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 146 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 10.3 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 10.3 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 10.3 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 10.3 K\n",
      "11 | lstm_encoder                       | LSTM                            | 20.4 K\n",
      "12 | lstm_decoder                       | LSTM                            | 20.4 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 5.1 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 100   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 12.8 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 6.1 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 5.2 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 10.3 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 5.2 K \n",
      "20 | output_layer                       | Linear                          | 51    \n",
      "----------------------------------------------------------------------------------------\n",
      "425 K     Trainable params\n",
      "0         Non-trainable params\n",
      "425 K     Total params\n",
      "1.701     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00,  6.08it/s, loss=0.536, train_loss_step=0.410, val_loss=0.378, train_loss_epoch=0.410]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00,  6.06it/s, loss=0.536, train_loss_step=0.410, val_loss=0.378, train_loss_epoch=0.410]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 16:59:48,819]\u001b[0m Trial 20 finished with value: 0.37797415256500244 and parameters: {'batch_size': 32, 'hidden_size': 50, 'attention_head_size': 4, 'dropout': 0.29602105572002796, 'learning_rate': 0.006539563220478549}. Best is trial 4 with value: 0.08354290574789047.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 5.3 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 188 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 186 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 16.8 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 16.8 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 16.8 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 16.8 K\n",
      "11 | lstm_encoder                       | LSTM                            | 33.3 K\n",
      "12 | lstm_decoder                       | LSTM                            | 33.3 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 8.3 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 128   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 20.9 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 10.9 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 8.4 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 16.8 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 8.4 K \n",
      "20 | output_layer                       | Linear                          | 65    \n",
      "----------------------------------------------------------------------------------------\n",
      "586 K     Trainable params\n",
      "0         Non-trainable params\n",
      "586 K     Total params\n",
      "2.344     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 4/4 [00:00<00:00,  5.70it/s, loss=0.214, train_loss_step=0.159, val_loss=0.199, train_loss_epoch=0.175]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 17:00:01,346]\u001b[0m Trial 21 finished with value: 0.19850240647792816 and parameters: {'batch_size': 16, 'hidden_size': 64, 'attention_head_size': 3, 'dropout': 0.2064456962988528, 'learning_rate': 0.001895743014866036}. Best is trial 4 with value: 0.08354290574789047.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.8 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 171 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 169 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 13.8 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 13.8 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 13.8 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 13.8 K\n",
      "11 | lstm_encoder                       | LSTM                            | 27.4 K\n",
      "12 | lstm_decoder                       | LSTM                            | 27.4 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 6.8 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 116   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 17.2 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 8.9 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 7.0 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 13.8 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 7.0 K \n",
      "20 | output_layer                       | Linear                          | 59    \n",
      "----------------------------------------------------------------------------------------\n",
      "514 K     Trainable params\n",
      "0         Non-trainable params\n",
      "514 K     Total params\n",
      "2.059     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 4/4 [00:00<00:00,  5.67it/s, loss=0.213, train_loss_step=0.146, val_loss=0.144, train_loss_epoch=0.170]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 17:00:12,857]\u001b[0m Trial 22 finished with value: 0.14441730082035065 and parameters: {'batch_size': 16, 'hidden_size': 58, 'attention_head_size': 3, 'dropout': 0.23042737154524665, 'learning_rate': 0.0026113741465503626}. Best is trial 4 with value: 0.08354290574789047.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 5.3 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 188 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 186 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 16.8 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 16.8 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 16.8 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 16.8 K\n",
      "11 | lstm_encoder                       | LSTM                            | 33.3 K\n",
      "12 | lstm_decoder                       | LSTM                            | 33.3 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 8.3 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 128   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 20.9 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 10.9 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 8.4 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 16.8 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 8.4 K \n",
      "20 | output_layer                       | Linear                          | 65    \n",
      "----------------------------------------------------------------------------------------\n",
      "586 K     Trainable params\n",
      "0         Non-trainable params\n",
      "586 K     Total params\n",
      "2.344     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 4/4 [00:00<00:00,  5.63it/s, loss=0.171, train_loss_step=0.144, val_loss=0.181, train_loss_epoch=0.140] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 17:00:23,568]\u001b[0m Trial 23 finished with value: 0.1814696192741394 and parameters: {'batch_size': 16, 'hidden_size': 64, 'attention_head_size': 3, 'dropout': 0.21487186015970328, 'learning_rate': 0.005247886456768439}. Best is trial 4 with value: 0.08354290574789047.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.8 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 171 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 169 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 13.8 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 13.8 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 13.8 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 13.8 K\n",
      "11 | lstm_encoder                       | LSTM                            | 27.4 K\n",
      "12 | lstm_decoder                       | LSTM                            | 27.4 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 6.8 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 116   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 17.2 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 8.9 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 7.0 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 13.8 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 7.0 K \n",
      "20 | output_layer                       | Linear                          | 59    \n",
      "----------------------------------------------------------------------------------------\n",
      "514 K     Trainable params\n",
      "0         Non-trainable params\n",
      "514 K     Total params\n",
      "2.059     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 4/4 [00:00<00:00,  5.68it/s, loss=0.265, train_loss_step=0.256, val_loss=0.214, train_loss_epoch=0.236]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 4/4 [00:00<00:00,  5.67it/s, loss=0.265, train_loss_step=0.256, val_loss=0.214, train_loss_epoch=0.236]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 17:00:38,195]\u001b[0m Trial 24 finished with value: 0.21383516490459442 and parameters: {'batch_size': 16, 'hidden_size': 58, 'attention_head_size': 3, 'dropout': 0.1867655966276005, 'learning_rate': 0.0009017791560890321}. Best is trial 4 with value: 0.08354290574789047.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 1.8 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 55.4 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 54.8 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K \n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 808   \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576   \n",
      "20 | output_layer                       | Linear                          | 17    \n",
      "----------------------------------------------------------------------------------------\n",
      "124 K     Trainable params\n",
      "0         Non-trainable params\n",
      "124 K     Total params\n",
      "0.499     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 4/4 [00:00<00:00,  7.75it/s, loss=0.278, train_loss_step=0.238, val_loss=0.196, train_loss_epoch=0.226]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 4/4 [00:00<00:00,  7.73it/s, loss=0.278, train_loss_step=0.238, val_loss=0.196, train_loss_epoch=0.226]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 17:00:48,684]\u001b[0m Trial 25 finished with value: 0.19580619037151337 and parameters: {'batch_size': 16, 'hidden_size': 16, 'attention_head_size': 2, 'dropout': 0.11521299183327335, 'learning_rate': 0.003371586671173494}. Best is trial 4 with value: 0.08354290574789047.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.2 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 145 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 144 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 9.9 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 9.9 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 9.9 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 9.9 K \n",
      "11 | lstm_encoder                       | LSTM                            | 19.6 K\n",
      "12 | lstm_decoder                       | LSTM                            | 19.6 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 4.9 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 98    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 12.3 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 6.4 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 5.0 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 9.9 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 5.0 K \n",
      "20 | output_layer                       | Linear                          | 50    \n",
      "----------------------------------------------------------------------------------------\n",
      "415 K     Trainable params\n",
      "0         Non-trainable params\n",
      "415 K     Total params\n",
      "1.661     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.99it/s, loss=0.279, train_loss_step=0.215, val_loss=0.254, train_loss_epoch=0.209] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 17:00:55,563]\u001b[0m Trial 26 finished with value: 0.2538076341152191 and parameters: {'batch_size': 16, 'hidden_size': 49, 'attention_head_size': 3, 'dropout': 0.15267554149583246, 'learning_rate': 0.00990113752180465}. Best is trial 4 with value: 0.08354290574789047.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.9 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 174 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 171 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 14.3 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 14.3 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 14.3 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 14.3 K\n",
      "11 | lstm_encoder                       | LSTM                            | 28.3 K\n",
      "12 | lstm_decoder                       | LSTM                            | 28.3 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 7.1 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 118   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 17.8 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 8.4 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 7.2 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 14.3 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 7.2 K \n",
      "20 | output_layer                       | Linear                          | 60    \n",
      "----------------------------------------------------------------------------------------\n",
      "525 K     Trainable params\n",
      "0         Non-trainable params\n",
      "525 K     Total params\n",
      "2.102     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 7/7 [00:01<00:00,  6.45it/s, loss=0.155, train_loss_step=0.0936, val_loss=0.0877, train_loss_epoch=0.143]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 17:01:13,087]\u001b[0m Trial 27 finished with value: 0.0877448320388794 and parameters: {'batch_size': 8, 'hidden_size': 59, 'attention_head_size': 4, 'dropout': 0.2553123927912326, 'learning_rate': 0.0017127537818602252}. Best is trial 4 with value: 0.08354290574789047.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 4.7 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 165 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 163 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 12.9 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 12.9 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 12.9 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 12.9 K\n",
      "11 | lstm_encoder                       | LSTM                            | 25.5 K\n",
      "12 | lstm_decoder                       | LSTM                            | 25.5 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 6.4 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 112   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 16.0 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 8.0 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 6.5 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 12.9 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 6.5 K \n",
      "20 | output_layer                       | Linear                          | 57    \n",
      "----------------------------------------------------------------------------------------\n",
      "491 K     Trainable params\n",
      "0         Non-trainable params\n",
      "491 K     Total params\n",
      "1.966     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00,  7.04it/s, loss=0.166, train_loss_step=0.136, val_loss=0.121, train_loss_epoch=0.152]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00,  7.03it/s, loss=0.166, train_loss_step=0.136, val_loss=0.121, train_loss_epoch=0.152]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 17:01:33,578]\u001b[0m Trial 28 finished with value: 0.1207784041762352 and parameters: {'batch_size': 8, 'hidden_size': 56, 'attention_head_size': 4, 'dropout': 0.26984035923091754, 'learning_rate': 0.0013684442453263679}. Best is trial 4 with value: 0.08354290574789047.\u001b[0m\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.6 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 123 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 122 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 7.0 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 7.0 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 7.0 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 7.0 K \n",
      "11 | lstm_encoder                       | LSTM                            | 13.8 K\n",
      "12 | lstm_decoder                       | LSTM                            | 13.8 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 3.4 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 82    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 8.7 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 4.2 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 3.5 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 7.0 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 3.5 K \n",
      "20 | output_layer                       | Linear                          | 42    \n",
      "----------------------------------------------------------------------------------------\n",
      "333 K     Trainable params\n",
      "0         Non-trainable params\n",
      "333 K     Total params\n",
      "1.335     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00,  7.30it/s, loss=0.327, train_loss_step=0.228, val_loss=0.101, train_loss_epoch=0.299]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00,  7.29it/s, loss=0.327, train_loss_step=0.228, val_loss=0.101, train_loss_epoch=0.299]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-04-05 17:01:53,095]\u001b[0m Trial 29 finished with value: 0.1008903905749321 and parameters: {'batch_size': 8, 'hidden_size': 41, 'attention_head_size': 4, 'dropout': 0.2545447585067546, 'learning_rate': 0.0003433546261363879}. Best is trial 4 with value: 0.08354290574789047.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Best SMAPE: 0.08354290574789047\n",
      "🎯 Best Params:\n",
      "  batch_size: 16\n",
      "  hidden_size: 63\n",
      "  attention_head_size: 3\n",
      "  dropout: 5.71315030161168e-05\n",
      "  learning_rate: 0.0013366258010220143\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", study_name=\"TFT_Diesel_Tuning\")\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Print results\n",
    "print(\"✅ Best SMAPE:\", study.best_value)\n",
    "print(\"🎯 Best Params:\")\n",
    "for k, v in study.best_params.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "train_loader = training.to_dataloader(train=True, batch_size=16, num_workers=0)\n",
    "val_loader = validation.to_dataloader(train=False, batch_size=16, num_workers=0)\n",
    "\n",
    "tft_diesel = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    hidden_size=63,\n",
    "    attention_head_size=3,\n",
    "    dropout=0.01,\n",
    "    learning_rate=0.0013366258010220143,\n",
    "    loss=SMAPE(),\n",
    "    log_interval=10,\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, verbose=True, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1     \n",
      "3  | prescalers                         | ModuleDict                      | 1.3 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 5.2 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 185 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 183 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 16.3 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 16.3 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 16.3 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 16.3 K\n",
      "11 | lstm_encoder                       | LSTM                            | 32.3 K\n",
      "12 | lstm_decoder                       | LSTM                            | 32.3 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 8.1 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 126   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 20.2 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 10.7 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 8.2 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 16.3 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 8.2 K \n",
      "20 | output_layer                       | Linear                          | 64    \n",
      "----------------------------------------------------------------------------------------\n",
      "574 K     Trainable params\n",
      "0         Non-trainable params\n",
      "574 K     Total params\n",
      "2.296     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  9.74it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/anaconda3/envs/fuel_env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 4/4 [00:01<00:00,  2.74it/s, loss=0.482, v_num=41, train_loss_step=0.473, val_loss=1.410]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 4/4 [00:01<00:00,  2.77it/s, loss=0.483, v_num=41, train_loss_step=0.402, val_loss=1.400, train_loss_epoch=0.482]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.006 >= min_delta = 0.0. New best score: 1.405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 4/4 [00:01<00:00,  2.86it/s, loss=0.475, v_num=41, train_loss_step=0.488, val_loss=1.320, train_loss_epoch=0.485]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.082 >= min_delta = 0.0. New best score: 1.323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 4/4 [00:01<00:00,  2.91it/s, loss=0.464, v_num=41, train_loss_step=0.542, val_loss=1.320, train_loss_epoch=0.459]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 1.322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 4/4 [00:01<00:00,  2.56it/s, loss=0.456, v_num=41, train_loss_step=0.402, val_loss=1.240, train_loss_epoch=0.430]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.082 >= min_delta = 0.0. New best score: 1.239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 4/4 [00:01<00:00,  2.92it/s, loss=0.45, v_num=41, train_loss_step=0.357, val_loss=1.230, train_loss_epoch=0.427] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 1.234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 4/4 [00:01<00:00,  2.50it/s, loss=0.438, v_num=41, train_loss_step=0.401, val_loss=1.130, train_loss_epoch=0.418]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.105 >= min_delta = 0.0. New best score: 1.130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 4/4 [00:01<00:00,  2.94it/s, loss=0.413, v_num=41, train_loss_step=0.466, val_loss=1.110, train_loss_epoch=0.388]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.018 >= min_delta = 0.0. New best score: 1.112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 4/4 [00:01<00:00,  2.81it/s, loss=0.397, v_num=41, train_loss_step=0.441, val_loss=0.958, train_loss_epoch=0.376]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.153 >= min_delta = 0.0. New best score: 0.958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:01<00:00,  2.91it/s, loss=0.386, v_num=41, train_loss_step=0.300, val_loss=0.955, train_loss_epoch=0.332]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 4/4 [00:01<00:00,  2.93it/s, loss=0.366, v_num=41, train_loss_step=0.280, val_loss=0.804, train_loss_epoch=0.320]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.150 >= min_delta = 0.0. New best score: 0.804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 4/4 [00:01<00:00,  2.37it/s, loss=0.34, v_num=41, train_loss_step=0.362, val_loss=0.801, train_loss_epoch=0.316] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 4/4 [00:01<00:00,  2.95it/s, loss=0.326, v_num=41, train_loss_step=0.283, val_loss=0.670, train_loss_epoch=0.291]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.131 >= min_delta = 0.0. New best score: 0.670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 4/4 [00:01<00:00,  2.92it/s, loss=0.306, v_num=41, train_loss_step=0.225, val_loss=0.666, train_loss_epoch=0.281]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 4/4 [00:01<00:00,  2.28it/s, loss=0.292, v_num=41, train_loss_step=0.291, val_loss=0.552, train_loss_epoch=0.263]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.114 >= min_delta = 0.0. New best score: 0.552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 4/4 [00:01<00:00,  2.94it/s, loss=0.267, v_num=41, train_loss_step=0.179, val_loss=0.463, train_loss_epoch=0.238]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.089 >= min_delta = 0.0. New best score: 0.463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 4/4 [00:01<00:00,  2.92it/s, loss=0.259, v_num=41, train_loss_step=0.187, val_loss=0.459, train_loss_epoch=0.245]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 4/4 [00:01<00:00,  2.94it/s, loss=0.247, v_num=41, train_loss_step=0.248, val_loss=0.368, train_loss_epoch=0.225]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.092 >= min_delta = 0.0. New best score: 0.368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 4/4 [00:01<00:00,  2.95it/s, loss=0.225, v_num=41, train_loss_step=0.215, val_loss=0.286, train_loss_epoch=0.194]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.082 >= min_delta = 0.0. New best score: 0.286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 4/4 [00:01<00:00,  2.85it/s, loss=0.217, v_num=41, train_loss_step=0.183, val_loss=0.278, train_loss_epoch=0.200]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.008 >= min_delta = 0.0. New best score: 0.278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 4/4 [00:01<00:00,  2.05it/s, loss=0.204, v_num=41, train_loss_step=0.136, val_loss=0.203, train_loss_epoch=0.188]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.075 >= min_delta = 0.0. New best score: 0.203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 4/4 [00:01<00:00,  2.88it/s, loss=0.19, v_num=41, train_loss_step=0.154, val_loss=0.156, train_loss_epoch=0.165] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.046 >= min_delta = 0.0. New best score: 0.156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 4/4 [00:01<00:00,  2.93it/s, loss=0.177, v_num=41, train_loss_step=0.148, val_loss=0.130, train_loss_epoch=0.166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.026 >= min_delta = 0.0. New best score: 0.130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 4/4 [00:01<00:00,  2.87it/s, loss=0.164, v_num=41, train_loss_step=0.147, val_loss=0.107, train_loss_epoch=0.144]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.024 >= min_delta = 0.0. New best score: 0.107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 4/4 [00:01<00:00,  2.93it/s, loss=0.151, v_num=41, train_loss_step=0.174, val_loss=0.103, train_loss_epoch=0.136]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 4/4 [00:01<00:00,  2.92it/s, loss=0.146, v_num=41, train_loss_step=0.150, val_loss=0.103, train_loss_epoch=0.138]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 4/4 [00:01<00:00,  2.94it/s, loss=0.14, v_num=41, train_loss_step=0.139, val_loss=0.103, train_loss_epoch=0.135] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 4/4 [00:01<00:00,  2.93it/s, loss=0.105, v_num=41, train_loss_step=0.104, val_loss=0.133, train_loss_epoch=0.0962] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 10 records. Best score: 0.103. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 4/4 [00:02<00:00,  1.97it/s, loss=0.105, v_num=41, train_loss_step=0.104, val_loss=0.133, train_loss_epoch=0.102] \n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"cpu\",\n",
    "    callbacks=[early_stop, lr_logger],\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "trainer.fit(tft_diesel, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Final Evaluation Metrics for Diesel (Optuna-Tuned):\n",
      "SMAPE: 0.13\n",
      "MAE: 50.57\n",
      "RMSE: 50.57\n"
     ]
    }
   ],
   "source": [
    "actuals = torch.cat([y[0] for x, y in iter(val_loader)])\n",
    "predictions = tft_diesel.predict(val_loader)\n",
    "\n",
    "y_true = actuals.detach().cpu().numpy()\n",
    "y_pred = predictions.detach().cpu().numpy()\n",
    "\n",
    "print(f\"\\n🔍 Final Evaluation Metrics for Diesel (Optuna-Tuned):\")\n",
    "print(f\"SMAPE: {SMAPE()(predictions, actuals).item():.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_true, y_pred):.2f}\")\n",
    "print(f\"RMSE: {mean_squared_error(y_true, y_pred, squared=False):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diesel residuals saved to 'tft_diesel_residuals.csv'\n"
     ]
    }
   ],
   "source": [
    "raw_predictions, x = tft_diesel.predict(val_loader, mode=\"raw\", return_x=True)\n",
    "\n",
    "# Computing residuals\n",
    "residuals = actuals - predictions\n",
    "residuals_np = residuals.detach().cpu().numpy().flatten()\n",
    "\n",
    "# Mapping decoder time_idx to real dates\n",
    "decoded_time = x[\"decoder_time_idx\"].detach().cpu().numpy().flatten()\n",
    "date_lookup = df.set_index(\"time_idx\")[\"Date\"]\n",
    "dates = pd.to_datetime([date_lookup[int(t)] for t in decoded_time])\n",
    "\n",
    "# Building and saving residual DataFrame\n",
    "res_df = pd.DataFrame({\n",
    "    \"date\": dates,\n",
    "    \"residuals\": residuals_np\n",
    "})\n",
    "\n",
    "res_df.to_csv(\"tft_diesel_residuals.csv\", index=False)\n",
    "print(\"Diesel residuals saved to 'tft_diesel_residuals.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fuel_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
